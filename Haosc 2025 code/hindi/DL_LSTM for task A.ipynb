{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blond-share",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "democratic-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f1cbd-f73e-44de-a402-cc11d26dd583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bdd8ce0-4c45-4f15-8ec6-a99db6336cde",
   "metadata": {},
   "source": [
    "# task_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-reynolds",
   "metadata": {},
   "source": [
    "### Handling Pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statutory-athens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_1817.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Ba8@ DaNn G@rainiD IR T३ PDBB WRHE W PRD BCEN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_7.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Nari nari mat kar pagle, Nari he nark ka dwar....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_1.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Kitni push ops maarsakte ho dafly? 5 aur agar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_32.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>अब इसमें मेरी कहां गलती है बताओ.. तरबूज़ वाली क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_1714.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>\"KUDI MENU KEHNDl... 'MENU JUTI LA DE SONIYE.....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id         task_1      task_3   task_4  \\\n",
       "0  Hindi_image_1817.jpg      Sarcastic      Vulgar  Abusive   \n",
       "1     Hindi_image_7.jpg  Non-Sarcastic      Vulgar  Abusive   \n",
       "2     Hindi_image_1.jpg      Sarcastic  Non Vulgar  Abusive   \n",
       "3    Hindi_image_32.jpg      Sarcastic      Vulgar  Abusive   \n",
       "4  Hindi_image_1714.jpg      Sarcastic  Non Vulgar  Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Ba8@ DaNn G@rainiD IR T३ PDBB WRHE W PRD BCEN ...  \n",
       "1  Nari nari mat kar pagle, Nari he nark ka dwar....  \n",
       "2  Kitni push ops maarsakte ho dafly? 5 aur agar ...  \n",
       "3  अब इसमें मेरी कहां गलती है बताओ.. तरबूज़ वाली क...  \n",
       "4  \"KUDI MENU KEHNDl... 'MENU JUTI LA DE SONIYE.....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocess_data.csv')\n",
    "data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0ca564-05fb-48d3-ae7f-131cd69197bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>Sign You are Bancho a] _ ~\"11|7 have best ffen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>एक महिला घोडे़ के लिंग लिया| घोड़ा उत्साहित हो...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>एक टीचर ने एक लड़के को पेपर में नक़ल करते पकड लि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>show me Sckht Launda Kisslay Jha CTrollerlzabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>पति सुहागरात में पत्नी की निप्पल चूसते हुए बोल...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id         task_1      task_3       task_4  \\\n",
       "0   Hindi_image_410.jpg      Sarcastic      Vulgar  Non-abusive   \n",
       "1   Hindi_image_114.jpg  Non-Sarcastic      Vulgar      Abusive   \n",
       "2   Hindi_image_101.jpg  Non-Sarcastic  Non Vulgar  Non-abusive   \n",
       "3  Hindi_image_1747.jpg      Sarcastic      Vulgar      Abusive   \n",
       "4    Hindi_image_19.jpg  Non-Sarcastic  Non Vulgar      Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Sign You are Bancho a] _ ~\"11|7 have best ffen...  \n",
       "1  एक महिला घोडे़ के लिंग लिया| घोड़ा उत्साहित हो...  \n",
       "2  एक टीचर ने एक लड़के को पेपर में नक़ल करते पकड लि...  \n",
       "3  show me Sckht Launda Kisslay Jha CTrollerlzabu...  \n",
       "4  पति सुहागरात में पत्नी की निप्पल चूसते हुए बोल...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../preprocess_test_data.csv') \n",
    "test_data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "test_data = test_data.drop(['Unnamed: 0'],axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "double-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text_clean'].astype(str)\n",
    "tokenizer = Tokenizer(num_words = 1500,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5f8c24-b908-4afc-a77c-b65abc97b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text_clean'].astype(str)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neither-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words :  8859\n",
      "[[   0    0    0 ... 1173 1174  571]\n",
      " [   0    0    0 ...   19   15 1176]\n",
      " [   0    0    0 ...    4  773   17]\n",
      " ...\n",
      " [   0    0    0 ...   36  377   30]\n",
      " [   0    0    0 ...   27  122  333]\n",
      " [   0    0    0 ...  118  739   89]]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 2500\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"No of unique words : \",len(index_of_words))\n",
    "\n",
    "X = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = data['task_1']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d48e4abf-9eeb-44c8-b4fa-d1ec6e90ea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   46  963    9]\n",
      " [   0    0    0 ...   45   35   26]\n",
      " [   0    0    0 ...  545  310    2]\n",
      " ...\n",
      " [   0    0    0 ...   87  143  333]\n",
      " [   0    0    0 ...    0    0  318]\n",
      " [   0    0    0 ...    0    0 1297]]\n"
     ]
    }
   ],
   "source": [
    "test_X = pad_sequences(test_sequence , maxlen = max_seq_len )\n",
    "test_Y = test_data['task_1']\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proved-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "vocabSize = len(index_of_words)\n",
    "lstm_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "graduate-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)\n",
    "Y_true = Y_test\n",
    "Y_train = pd.get_dummies(Y_train).values\n",
    "Y_test = pd.get_dummies(Y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec81820-f642-4292-8db1-6900712764ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Y: [[False  True]\n",
      " [ True False]\n",
      " [ True False]\n",
      " ...\n",
      " [False  True]\n",
      " [ True False]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_true = test_Y\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "print(\"test_Y:\", test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8c676-4add-45b2-a277-3aab359aa12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8242382b-a57a-4afb-9c5f-03ea4c65ecc9",
   "metadata": {},
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1586ad0b-5f4a-490d-936a-a85bad2d1b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 1.5781758957654723, 1: 0.7318731117824774}\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# ===== 类别权重计算 =====\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compact-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         2267904   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2500, 256)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2354626 (8.98 MB)\n",
      "Trainable params: 2354498 (8.98 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim,input_length = 2500))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "universal-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_a1.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fifteen-christianity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7424 - accuracy: 0.5449\n",
      "Epoch 1: val_loss improved from inf to 0.69344, saving model to hasoc_a1.h5\n",
      "31/31 [==============================] - 166s 5s/step - loss: 0.7424 - accuracy: 0.5449 - val_loss: 0.6934 - val_accuracy: 0.5116\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.6656\n",
      "Epoch 2: val_loss improved from 0.69344 to 0.68652, saving model to hasoc_a1.h5\n",
      "31/31 [==============================] - 160s 5s/step - loss: 0.6006 - accuracy: 0.6656 - val_loss: 0.6865 - val_accuracy: 0.6047\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.7626\n",
      "Epoch 3: val_loss did not improve from 0.68652\n",
      "31/31 [==============================] - 163s 5s/step - loss: 0.4846 - accuracy: 0.7626 - val_loss: 0.6887 - val_accuracy: 0.5349\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3585 - accuracy: 0.8142\n",
      "Epoch 4: val_loss improved from 0.68652 to 0.67687, saving model to hasoc_a1.h5\n",
      "31/31 [==============================] - 165s 5s/step - loss: 0.3585 - accuracy: 0.8142 - val_loss: 0.6769 - val_accuracy: 0.6279\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.8586\n",
      "Epoch 5: val_loss improved from 0.67687 to 0.66471, saving model to hasoc_a1.h5\n",
      "31/31 [==============================] - 165s 5s/step - loss: 0.3003 - accuracy: 0.8586 - val_loss: 0.6647 - val_accuracy: 0.6279\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.8854\n",
      "Epoch 6: val_loss did not improve from 0.66471\n",
      "31/31 [==============================] - 165s 5s/step - loss: 0.2382 - accuracy: 0.8854 - val_loss: 0.6689 - val_accuracy: 0.6279\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9061\n",
      "Epoch 7: val_loss did not improve from 0.66471\n",
      "31/31 [==============================] - 165s 5s/step - loss: 0.1926 - accuracy: 0.9061 - val_loss: 0.6932 - val_accuracy: 0.6279\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9195\n",
      "Epoch 8: val_loss did not improve from 0.66471\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "31/31 [==============================] - 164s 5s/step - loss: 0.1691 - accuracy: 0.9195 - val_loss: 0.7333 - val_accuracy: 0.6279\n",
      "Epoch 8: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x285159767c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, validation_data=(X_test,Y_test), class_weight=class_weight_dict, callbacks=[checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "conceptual-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 679ms/step - loss: 0.6647 - accuracy: 0.6279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6647120714187622, 0.6279069781303406]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_a1.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "facial-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 17s 657ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "right-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Sarcastic':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "anticipated-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.03      0.05       676\n",
      "           1       0.12      0.96      0.21        93\n",
      "\n",
      "    accuracy                           0.14       769\n",
      "   macro avg       0.47      0.49      0.13       769\n",
      "weighted avg       0.73      0.14      0.07       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "combined-reduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [[0.4288956  0.5711044 ]\n",
      " [0.34199494 0.65800506]\n",
      " [0.38701785 0.61298215]\n",
      " ...\n",
      " [0.31160644 0.6883936 ]\n",
      " [0.43951362 0.5604863 ]\n",
      " [0.4552256  0.5447745 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_pred:\", Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "large-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_class: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fe1418d-8cba-4651-ac94-70dd0776578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Sarcastic')\n",
    "    else :\n",
    "        pred_actual.append('Non-Sarcastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e917946f-7d5c-44b5-8452-3bba3ee71967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id      label\n",
       "0   Hindi_image_410.jpg  Sarcastic\n",
       "1   Hindi_image_114.jpg  Sarcastic\n",
       "2   Hindi_image_101.jpg  Sarcastic\n",
       "3  Hindi_image_1747.jpg  Sarcastic\n",
       "4    Hindi_image_19.jpg  Sarcastic"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_a.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59806c35-9a98-41b3-b364-41286b31e3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc7be77d-02bf-4ef9-a0fd-5f81c4e1dc6d",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2307fc-07fb-4782-b672-55796d12fecd",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0dd4207-d9ae-4744-a324-e83af8656d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1321a6-609c-452e-a64a-22769876ccf2",
   "metadata": {},
   "source": [
    "# Handling Pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae67c851-e9b6-45f7-9717-a75f5c68b337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_1817.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Ba8@ DaNn G@rainiD IR T३ PDBB WRHE W PRD BCEN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_7.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Nari nari mat kar pagle, Nari he nark ka dwar....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_1.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Kitni push ops maarsakte ho dafly? 5 aur agar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_32.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>अब इसमें मेरी कहां गलती है बताओ.. तरबूज़ वाली क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_1714.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>\"KUDI MENU KEHNDl... 'MENU JUTI LA DE SONIYE.....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id         task_1      task_3   task_4  \\\n",
       "0  Hindi_image_1817.jpg      Sarcastic      Vulgar  Abusive   \n",
       "1     Hindi_image_7.jpg  Non-Sarcastic      Vulgar  Abusive   \n",
       "2     Hindi_image_1.jpg      Sarcastic  Non Vulgar  Abusive   \n",
       "3    Hindi_image_32.jpg      Sarcastic      Vulgar  Abusive   \n",
       "4  Hindi_image_1714.jpg      Sarcastic  Non Vulgar  Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Ba8@ DaNn G@rainiD IR T३ PDBB WRHE W PRD BCEN ...  \n",
       "1  Nari nari mat kar pagle, Nari he nark ka dwar....  \n",
       "2  Kitni push ops maarsakte ho dafly? 5 aur agar ...  \n",
       "3  अब इसमें मेरी कहां गलती है बताओ.. तरबूज़ वाली क...  \n",
       "4  \"KUDI MENU KEHNDl... 'MENU JUTI LA DE SONIYE.....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocess_data.csv')\n",
    "data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e07c71e-824e-4dad-a769-575a67570810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>Sign You are Bancho a] _ ~\"11|7 have best ffen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>एक महिला घोडे़ के लिंग लिया| घोड़ा उत्साहित हो...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>एक टीचर ने एक लड़के को पेपर में नक़ल करते पकड लि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>show me Sckht Launda Kisslay Jha CTrollerlzabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>पति सुहागरात में पत्नी की निप्पल चूसते हुए बोल...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id         task_1      task_3       task_4  \\\n",
       "0   Hindi_image_410.jpg      Sarcastic      Vulgar  Non-abusive   \n",
       "1   Hindi_image_114.jpg  Non-Sarcastic      Vulgar      Abusive   \n",
       "2   Hindi_image_101.jpg  Non-Sarcastic  Non Vulgar  Non-abusive   \n",
       "3  Hindi_image_1747.jpg      Sarcastic      Vulgar      Abusive   \n",
       "4    Hindi_image_19.jpg  Non-Sarcastic  Non Vulgar      Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Sign You are Bancho a] _ ~\"11|7 have best ffen...  \n",
       "1  एक महिला घोडे़ के लिंग लिया| घोड़ा उत्साहित हो...  \n",
       "2  एक टीचर ने एक लड़के को पेपर में नक़ल करते पकड लि...  \n",
       "3  show me Sckht Launda Kisslay Jha CTrollerlzabu...  \n",
       "4  पति सुहागरात में पत्नी की निप्पल चूसते हुए बोल...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../preprocess_test_data.csv') \n",
    "test_data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "test_data = test_data.drop(['Unnamed: 0'],axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4bdd5b-bd00-491d-9de7-55cdd06e9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text_clean'].astype(str)\n",
    "tokenizer = Tokenizer(num_words = 1500,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52c863b-9570-4a44-af78-b9dbd048cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text_clean'].astype(str)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53bdf448-3adc-4cb7-9878-6a41ce8b97b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words :  8859\n",
      "[[   0    0    0 ... 1173 1174  571]\n",
      " [   0    0    0 ...   19   15 1176]\n",
      " [   0    0    0 ...    4  773   17]\n",
      " ...\n",
      " [   0    0    0 ...   36  377   30]\n",
      " [   0    0    0 ...   27  122  333]\n",
      " [   0    0    0 ...  118  739   89]]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 2500\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"No of unique words : \",len(index_of_words))\n",
    "\n",
    "X = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = data['task_1']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804074f8-3630-4517-a593-6261d2acde6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   46  963    9]\n",
      " [   0    0    0 ...   45   35   26]\n",
      " [   0    0    0 ...  545  310    2]\n",
      " ...\n",
      " [   0    0    0 ...   87  143  333]\n",
      " [   0    0    0 ...    0    0  318]\n",
      " [   0    0    0 ...    0    0 1297]]\n"
     ]
    }
   ],
   "source": [
    "test_X = pad_sequences(test_sequence , maxlen = max_seq_len )\n",
    "test_Y = test_data['task_1']\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1adb9194-3e45-4ed6-a520-38899bd30cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "vocabSize = len(index_of_words)\n",
    "lstm_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4c0e25-9c8d-4f02-8a1a-8fec81794aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)\n",
    "Y_true = Y_test\n",
    "Y_train = pd.get_dummies(Y_train).values\n",
    "Y_test = pd.get_dummies(Y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64b7e1b-368e-4044-84cb-c59de237971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Y: [[False  True]\n",
      " [ True False]\n",
      " [ True False]\n",
      " ...\n",
      " [False  True]\n",
      " [ True False]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_true = test_Y\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "print(\"test_Y:\", test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc20ae1-961d-4ac2-8dba-045c24a75e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         2267904   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2500, 256)         0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128)               164352    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2441154 (9.31 MB)\n",
      "Trainable params: 2440898 (9.31 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding 层\n",
    "model.add(Embedding(input_dim=vocabSize, output_dim=embed_dim, input_length=2500))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 双向 LSTM + 正则化\n",
    "model.add(Bidirectional(LSTM(units=lstm_out, dropout=0.4, recurrent_dropout=0.3)))\n",
    "\n",
    "# BN 层\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 全连接 + Dropout\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 输出层（sigmoid）\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# 编译\n",
    "optimizer = Adam(learning_rate=1e-4)  # 学习率调低\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c25da0-18d1-4c35-a1c0-58d9d0d631ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_b1.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b87686aa-87ee-4ca1-bfa1-2274bc4b2617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.8049 - accuracy: 0.5005 \n",
      "Epoch 1: val_loss improved from inf to 0.69149, saving model to hasoc_b1.h5\n",
      "31/31 [==============================] - 554s 18s/step - loss: 0.8049 - accuracy: 0.5005 - val_loss: 0.6915 - val_accuracy: 0.5407 - lr: 1.0000e-04\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 0.7757 - accuracy: 0.5139 \n",
      "Epoch 2: val_loss improved from 0.69149 to 0.69063, saving model to hasoc_b1.h5\n",
      "31/31 [==============================] - 564s 18s/step - loss: 0.7757 - accuracy: 0.5139 - val_loss: 0.6906 - val_accuracy: 0.5698 - lr: 1.0000e-04\n",
      "Epoch 3/5\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7896 - accuracy: 0.4995 \n",
      "Epoch 3: val_loss did not improve from 0.69063\n",
      "31/31 [==============================] - 571s 18s/step - loss: 0.7896 - accuracy: 0.4995 - val_loss: 0.6910 - val_accuracy: 0.5523 - lr: 1.0000e-04\n",
      "Epoch 4/5\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7581 - accuracy: 0.5304 \n",
      "Epoch 4: val_loss improved from 0.69063 to 0.69034, saving model to hasoc_b1.h5\n",
      "31/31 [==============================] - 614s 20s/step - loss: 0.7581 - accuracy: 0.5304 - val_loss: 0.6903 - val_accuracy: 0.5988 - lr: 1.0000e-04\n",
      "Epoch 5/5\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7605 - accuracy: 0.5655 \n",
      "Epoch 5: val_loss improved from 0.69034 to 0.69008, saving model to hasoc_b1.h5\n",
      "31/31 [==============================] - 617s 20s/step - loss: 0.7605 - accuracy: 0.5655 - val_loss: 0.6901 - val_accuracy: 0.5581 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22305959460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# 假设Y_train已独热编码\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 5, class_weight=class_weight_dict, validation_data=(X_test, Y_test), callbacks=[checkpoint, early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7140c1c0-5fcf-49f0-a072-5caaf17f4e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 9s 1s/step - loss: 0.6901 - accuracy: 0.5581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6900755167007446, 0.5581395626068115]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_b1.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e323eb-5a4c-4a91-bb38-e376ec22ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 39s 2s/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7716b56-3160-4948-90fa-c68ee63d0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i =='Sarcastic':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2d780a-5cc0-49d3-97a3-92ea30d5bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.21      0.35       676\n",
      "           1       0.14      0.92      0.24        93\n",
      "\n",
      "    accuracy                           0.30       769\n",
      "   macro avg       0.55      0.57      0.29       769\n",
      "weighted avg       0.85      0.30      0.33       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0b2f431-a8ca-4f85-962d-87d132a14b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Sarcastic')\n",
    "    else :\n",
    "        pred_actual.append('Non-Sarcastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3af96656-0fad-4a5f-bf59-4407ed406480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id      label\n",
       "0   Hindi_image_410.jpg  Sarcastic\n",
       "1   Hindi_image_114.jpg  Sarcastic\n",
       "2   Hindi_image_101.jpg  Sarcastic\n",
       "3  Hindi_image_1747.jpg  Sarcastic\n",
       "4    Hindi_image_19.jpg  Sarcastic"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_b.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733e3d1-1faa-4946-872f-1a44850d2179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a08d046a-48cf-45c4-8ca3-e656973ee255",
   "metadata": {},
   "source": [
    "# MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7e6ffd-c3fd-4a7e-8091-091c49418795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_c1.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe56d8e-a2ea-4275-8c7f-03954703df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.4283 \n",
      "Epoch 1: val_loss improved from inf to 0.70045, saving model to hasoc_c1.h5\n",
      "16/16 [==============================] - 228s 14s/step - loss: 0.6917 - accuracy: 0.4283 - val_loss: 0.7005 - val_accuracy: 0.4942\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 0.6644 - accuracy: 0.6997 \n",
      "Epoch 2: val_loss did not improve from 0.70045\n",
      "16/16 [==============================] - 224s 14s/step - loss: 0.6644 - accuracy: 0.6997 - val_loss: 0.7062 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5959 - accuracy: 0.7183 \n",
      "Epoch 3: val_loss improved from 0.70045 to 0.69831, saving model to hasoc_c1.h5\n",
      "16/16 [==============================] - 223s 14s/step - loss: 0.5959 - accuracy: 0.7183 - val_loss: 0.6983 - val_accuracy: 0.5756\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4468 - accuracy: 0.8400 \n",
      "Epoch 4: val_loss did not improve from 0.69831\n",
      "16/16 [==============================] - 225s 14s/step - loss: 0.4468 - accuracy: 0.8400 - val_loss: 1.1789 - val_accuracy: 0.5872\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.8483 \n",
      "Epoch 5: val_loss did not improve from 0.69831\n",
      "16/16 [==============================] - 231s 14s/step - loss: 0.3641 - accuracy: 0.8483 - val_loss: 0.9471 - val_accuracy: 0.4884\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.8865 \n",
      "Epoch 6: val_loss did not improve from 0.69831\n",
      "16/16 [==============================] - 222s 14s/step - loss: 0.2452 - accuracy: 0.8865 - val_loss: 1.1625 - val_accuracy: 0.5349\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1768 - accuracy: 0.9257 \n",
      "Epoch 7: val_loss did not improve from 0.69831\n",
      "16/16 [==============================] - 227s 14s/step - loss: 0.1768 - accuracy: 0.9257 - val_loss: 1.2986 - val_accuracy: 0.5291\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9319 \n",
      "Epoch 8: val_loss did not improve from 0.69831\n",
      "16/16 [==============================] - 229s 14s/step - loss: 0.1430 - accuracy: 0.9319 - val_loss: 1.3832 - val_accuracy: 0.5814\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9401 \n",
      "Epoch 9: val_loss did not improve from 0.69831\n",
      "16/16 [==============================] - 231s 14s/step - loss: 0.1397 - accuracy: 0.9401 - val_loss: 1.6415 - val_accuracy: 0.5291\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9360 \n",
      "Epoch 10: val_loss did not improve from 0.69831\n",
      "16/16 [==============================] - 230s 14s/step - loss: 0.1140 - accuracy: 0.9360 - val_loss: 1.7109 - val_accuracy: 0.5523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2249fa18880>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim, input_length=2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# 假设Y_train已独热编码\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64, class_weight=class_weight_dict, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5f9171-7289-402d-b574-1c634d3d8255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 690ms/step - loss: 0.6983 - accuracy: 0.5756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6983127593994141, 0.5755813717842102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_c1.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f57872-f149-445b-b8b2-18a2e9ad2434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 18s 700ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b667f062-ade2-4303-8de7-306cdc49d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i =='Sarcastic':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff53f8e2-b97e-4e54-86fa-769aa2dfd688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.23      0.37       676\n",
      "           1       0.13      0.86      0.23        93\n",
      "\n",
      "    accuracy                           0.30       769\n",
      "   macro avg       0.53      0.54      0.30       769\n",
      "weighted avg       0.83      0.30      0.35       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0d868de-ab46-4001-8bee-04ef1413ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Sarcastic')\n",
    "    else :\n",
    "        pred_actual.append('Non-Sarcastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee91e634-726e-44d2-8400-cb484a98db13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id      label\n",
       "0   Hindi_image_410.jpg  Sarcastic\n",
       "1   Hindi_image_114.jpg  Sarcastic\n",
       "2   Hindi_image_101.jpg  Sarcastic\n",
       "3  Hindi_image_1747.jpg  Sarcastic\n",
       "4    Hindi_image_19.jpg  Sarcastic"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_c.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2f376-7334-46b2-9f55-c929cca89630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0286b57-debc-4fcc-b090-24fa9b2044e4",
   "metadata": {},
   "source": [
    "# task_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ae5787-8d3e-4b7f-8fde-762df5c4a3b3",
   "metadata": {},
   "source": [
    "### Handling Pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05956797-704a-42ec-84ee-c61bfa5a089c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_1817.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Ba8@ DaNn G@rainiD IR T३ PDBB WRHE W PRD BCEN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_7.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Nari nari mat kar pagle, Nari he nark ka dwar....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_1.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Kitni push ops maarsakte ho dafly? 5 aur agar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_32.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>अब इसमें मेरी कहां गलती है बताओ.. तरबूज़ वाली क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_1714.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>\"KUDI MENU KEHNDl... 'MENU JUTI LA DE SONIYE.....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id         task_1      task_3   task_4  \\\n",
       "0  Hindi_image_1817.jpg      Sarcastic      Vulgar  Abusive   \n",
       "1     Hindi_image_7.jpg  Non-Sarcastic      Vulgar  Abusive   \n",
       "2     Hindi_image_1.jpg      Sarcastic  Non Vulgar  Abusive   \n",
       "3    Hindi_image_32.jpg      Sarcastic      Vulgar  Abusive   \n",
       "4  Hindi_image_1714.jpg      Sarcastic  Non Vulgar  Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Ba8@ DaNn G@rainiD IR T३ PDBB WRHE W PRD BCEN ...  \n",
       "1  Nari nari mat kar pagle, Nari he nark ka dwar....  \n",
       "2  Kitni push ops maarsakte ho dafly? 5 aur agar ...  \n",
       "3  अब इसमें मेरी कहां गलती है बताओ.. तरबूज़ वाली क...  \n",
       "4  \"KUDI MENU KEHNDl... 'MENU JUTI LA DE SONIYE.....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocess_data.csv')\n",
    "data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92b33b6-c5e7-4c4c-804f-61b39bdbba94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>Sign You are Bancho a] _ ~\"11|7 have best ffen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>एक महिला घोडे़ के लिंग लिया| घोड़ा उत्साहित हो...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>एक टीचर ने एक लड़के को पेपर में नक़ल करते पकड लि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>show me Sckht Launda Kisslay Jha CTrollerlzabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>पति सुहागरात में पत्नी की निप्पल चूसते हुए बोल...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id         task_1      task_3       task_4  \\\n",
       "0   Hindi_image_410.jpg      Sarcastic      Vulgar  Non-abusive   \n",
       "1   Hindi_image_114.jpg  Non-Sarcastic      Vulgar      Abusive   \n",
       "2   Hindi_image_101.jpg  Non-Sarcastic  Non Vulgar  Non-abusive   \n",
       "3  Hindi_image_1747.jpg      Sarcastic      Vulgar      Abusive   \n",
       "4    Hindi_image_19.jpg  Non-Sarcastic  Non Vulgar      Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Sign You are Bancho a] _ ~\"11|7 have best ffen...  \n",
       "1  एक महिला घोडे़ के लिंग लिया| घोड़ा उत्साहित हो...  \n",
       "2  एक टीचर ने एक लड़के को पेपर में नक़ल करते पकड लि...  \n",
       "3  show me Sckht Launda Kisslay Jha CTrollerlzabu...  \n",
       "4  पति सुहागरात में पत्नी की निप्पल चूसते हुए बोल...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../preprocess_test_data.csv') \n",
    "test_data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "test_data = test_data.drop(['Unnamed: 0'],axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e4d900-8089-46bc-8658-11179d89a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text_clean'].astype(str)\n",
    "tokenizer = Tokenizer(num_words = 1500,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82373895-3076-47a5-9e5a-8ea4f284bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text_clean'].astype(str)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2d6ad8-f4c5-4c18-8329-13af86bed38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words :  8859\n",
      "[[   0    0    0 ... 1173 1174  571]\n",
      " [   0    0    0 ...   19   15 1176]\n",
      " [   0    0    0 ...    4  773   17]\n",
      " ...\n",
      " [   0    0    0 ...   36  377   30]\n",
      " [   0    0    0 ...   27  122  333]\n",
      " [   0    0    0 ...  118  739   89]]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 2500\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"No of unique words : \",len(index_of_words))\n",
    "\n",
    "X = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = data['task_3']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27aac259-2197-4794-b7ef-7a714ae8e70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   46  963    9]\n",
      " [   0    0    0 ...   45   35   26]\n",
      " [   0    0    0 ...  545  310    2]\n",
      " ...\n",
      " [   0    0    0 ...   87  143  333]\n",
      " [   0    0    0 ...    0    0  318]\n",
      " [   0    0    0 ...    0    0 1297]]\n"
     ]
    }
   ],
   "source": [
    "test_X = pad_sequences(test_sequence , maxlen = max_seq_len )\n",
    "test_Y = test_data['task_3']\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd3016d-f98b-4094-8ffd-2b3ab0200aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "vocabSize = len(index_of_words)\n",
    "lstm_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576ef910-1779-4c6b-b748-a12055603f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)\n",
    "Y_true = Y_test\n",
    "Y_train = pd.get_dummies(Y_train).values\n",
    "Y_test = pd.get_dummies(Y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db9123f-a3b5-43fb-a040-838179bc8ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Y: [[False  True]\n",
      " [False  True]\n",
      " [ True False]\n",
      " ...\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_true = test_Y\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "print(\"test_Y:\",test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bebfa7f-5a34-4e39-9dab-73fb1636ac20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ad9b14e-6ada-43aa-b12c-fa9e40808a34",
   "metadata": {},
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "822350b4-96ca-4bfd-959a-701da7015f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         2267904   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2350210 (8.97 MB)\n",
      "Trainable params: 2350210 (8.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim,input_length = 2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b1e84b-048e-462c-869d-f52c1e4ab2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_a3.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afb092ef-440d-478f-b11f-af8cfd3b4895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6441 - accuracy: 0.6471\n",
      "Epoch 1: val_loss improved from inf to 0.61600, saving model to hasoc_a3.h5\n",
      "31/31 [==============================] - 127s 4s/step - loss: 0.6441 - accuracy: 0.6471 - val_loss: 0.6160 - val_accuracy: 0.6919\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 0.5797 - accuracy: 0.7152\n",
      "Epoch 2: val_loss improved from 0.61600 to 0.58734, saving model to hasoc_a3.h5\n",
      "31/31 [==============================] - 124s 4s/step - loss: 0.5797 - accuracy: 0.7152 - val_loss: 0.5873 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.7781\n",
      "Epoch 3: val_loss did not improve from 0.58734\n",
      "31/31 [==============================] - 126s 4s/step - loss: 0.4869 - accuracy: 0.7781 - val_loss: 0.6010 - val_accuracy: 0.7035\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.8617\n",
      "Epoch 4: val_loss did not improve from 0.58734\n",
      "31/31 [==============================] - 130s 4s/step - loss: 0.3237 - accuracy: 0.8617 - val_loss: 0.7202 - val_accuracy: 0.6686\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 0.9061\n",
      "Epoch 5: val_loss did not improve from 0.58734\n",
      "31/31 [==============================] - 128s 4s/step - loss: 0.2179 - accuracy: 0.9061 - val_loss: 0.8123 - val_accuracy: 0.6860\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1761 - accuracy: 0.9340\n",
      "Epoch 6: val_loss did not improve from 0.58734\n",
      "31/31 [==============================] - 130s 4s/step - loss: 0.1761 - accuracy: 0.9340 - val_loss: 0.9509 - val_accuracy: 0.6919\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9525\n",
      "Epoch 7: val_loss did not improve from 0.58734\n",
      "31/31 [==============================] - 129s 4s/step - loss: 0.1318 - accuracy: 0.9525 - val_loss: 1.1951 - val_accuracy: 0.6570\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9536\n",
      "Epoch 8: val_loss did not improve from 0.58734\n",
      "31/31 [==============================] - 130s 4s/step - loss: 0.1104 - accuracy: 0.9536 - val_loss: 1.1720 - val_accuracy: 0.6860\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9659\n",
      "Epoch 9: val_loss did not improve from 0.58734\n",
      "31/31 [==============================] - 130s 4s/step - loss: 0.0888 - accuracy: 0.9659 - val_loss: 1.3742 - val_accuracy: 0.6744\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9732\n",
      "Epoch 10: val_loss did not improve from 0.58734\n",
      "31/31 [==============================] - 130s 4s/step - loss: 0.0791 - accuracy: 0.9732 - val_loss: 1.5040 - val_accuracy: 0.6686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c561745820>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10 , validation_data = (X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4943fa-fa39-42ed-bf73-b7325a29c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 628ms/step - loss: 0.5873 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5873427987098694, 0.75]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_a3.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0e29fbc-f762-47b9-9bea-cf5d95228aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 17s 690ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd36ff72-bf5e-432f-87c2-b4c547b73ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i =='Vulgar':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3234470d-e10f-4bc1-b3d6-9823352f0009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       708\n",
      "           1       0.03      0.03      0.03        61\n",
      "\n",
      "    accuracy                           0.83       769\n",
      "   macro avg       0.47      0.47      0.47       769\n",
      "weighted avg       0.85      0.83      0.84       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e68890e2-59ca-4469-90a4-4fbff2622b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [[0.8319924  0.16800761]\n",
      " [0.28091756 0.7190825 ]\n",
      " [0.41645622 0.5835437 ]\n",
      " ...\n",
      " [0.9059605  0.09403948]\n",
      " [0.9685721  0.03142795]\n",
      " [0.9639709  0.03602908]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_pred:\", Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64b26cb2-5b79-4d11-b769-0ee9e9fc70ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_class: [0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df081796-36a9-49db-b8fa-e31189c15bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Vulgar')\n",
    "    else :\n",
    "        pred_actual.append('Non Vulgar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f52c0b6-2a89-49cf-a2e0-8d50993e9b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id       label\n",
       "0   Hindi_image_410.jpg  Non Vulgar\n",
       "1   Hindi_image_114.jpg      Vulgar\n",
       "2   Hindi_image_101.jpg      Vulgar\n",
       "3  Hindi_image_1747.jpg  Non Vulgar\n",
       "4    Hindi_image_19.jpg      Vulgar"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_a3.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb051c8c-e84b-4716-aaa9-77d305352ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0e00ed0-77bd-4941-8355-255cea587f50",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e35c21-bf9b-483f-8a02-5f363603bd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         2267904   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2500, 256)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2350466 (8.97 MB)\n",
      "Trainable params: 2350338 (8.97 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding层，增加防止过拟合的Dropout\n",
    "model.add(Embedding(input_dim=vocabSize, output_dim=embed_dim, input_length=2500))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM层，增加recurrent_dropout 和 output dropout\n",
    "model.add(LSTM(units=lstm_out, dropout=0.3, recurrent_dropout=0.3, return_sequences=False))\n",
    "\n",
    "# Batch Normalization增强泛化\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 全连接层，Softmax输出2分类，建议用categorical_crossentropy\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译\n",
    "optimizer = Adam(learning_rate=0.001)  # 学习率也可调整\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64df7480-48f1-4789-9c46-15253e70afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_b3.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab13b8c-eae0-4d60-8bb2-068b25c188b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.8500 - accuracy: 0.5181\n",
      "Epoch 1: val_loss improved from inf to 0.63237, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 129s 4s/step - loss: 0.8500 - accuracy: 0.5181 - val_loss: 0.6324 - val_accuracy: 0.6919\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 0.6653 - accuracy: 0.6264\n",
      "Epoch 2: val_loss improved from 0.63237 to 0.61393, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 124s 4s/step - loss: 0.6653 - accuracy: 0.6264 - val_loss: 0.6139 - val_accuracy: 0.6919\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.7100\n",
      "Epoch 3: val_loss improved from 0.61393 to 0.61031, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 126s 4s/step - loss: 0.5783 - accuracy: 0.7100 - val_loss: 0.6103 - val_accuracy: 0.6919\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.7472\n",
      "Epoch 4: val_loss improved from 0.61031 to 0.60756, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 128s 4s/step - loss: 0.5178 - accuracy: 0.7472 - val_loss: 0.6076 - val_accuracy: 0.6919\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.7564\n",
      "Epoch 5: val_loss improved from 0.60756 to 0.60431, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 128s 4s/step - loss: 0.4965 - accuracy: 0.7564 - val_loss: 0.6043 - val_accuracy: 0.6919\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.7946\n",
      "Epoch 6: val_loss improved from 0.60431 to 0.60421, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 130s 4s/step - loss: 0.4620 - accuracy: 0.7946 - val_loss: 0.6042 - val_accuracy: 0.6919\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4244 - accuracy: 0.8215\n",
      "Epoch 7: val_loss improved from 0.60421 to 0.60018, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 129s 4s/step - loss: 0.4244 - accuracy: 0.8215 - val_loss: 0.6002 - val_accuracy: 0.6919\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3625 - accuracy: 0.8349\n",
      "Epoch 8: val_loss improved from 0.60018 to 0.59818, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 129s 4s/step - loss: 0.3625 - accuracy: 0.8349 - val_loss: 0.5982 - val_accuracy: 0.6919\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3263 - accuracy: 0.8638\n",
      "Epoch 9: val_loss improved from 0.59818 to 0.58181, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 130s 4s/step - loss: 0.3263 - accuracy: 0.8638 - val_loss: 0.5818 - val_accuracy: 0.7093\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.8483\n",
      "Epoch 10: val_loss improved from 0.58181 to 0.57877, saving model to hasoc_b3.h5\n",
      "31/31 [==============================] - 129s 4s/step - loss: 0.3392 - accuracy: 0.8483 - val_loss: 0.5788 - val_accuracy: 0.7093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eacd5f0b50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, validation_data = (X_test,Y_test), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978bad34-9db4-4a5d-a179-caafaa6bc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 543ms/step - loss: 0.5788 - accuracy: 0.7093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5787697434425354, 0.7093023061752319]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_b3.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63e44e4b-fb3e-4276-8531-9821ffc0cd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 15s 609ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55111daf-b3bb-4323-bbbd-92c6cd848d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i =='Vulgar':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292b8923-2f0c-482f-9ee4-10c0335fb841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       708\n",
      "           1       0.00      0.00      0.00        61\n",
      "\n",
      "    accuracy                           0.89       769\n",
      "   macro avg       0.46      0.49      0.47       769\n",
      "weighted avg       0.85      0.89      0.87       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c813c60-ea56-49e8-8e43-c4055d0eb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Vulgar')\n",
    "    else :\n",
    "        pred_actual.append('Non Vulgar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8d9c480-235d-41bb-9833-90cea14f862d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id       label\n",
       "0   Hindi_image_410.jpg  Non Vulgar\n",
       "1   Hindi_image_114.jpg  Non Vulgar\n",
       "2   Hindi_image_101.jpg  Non Vulgar\n",
       "3  Hindi_image_1747.jpg  Non Vulgar\n",
       "4    Hindi_image_19.jpg  Non Vulgar"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_b3.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedaf193-aed8-4fc0-80ed-aba2e91e6251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39a702c8-5acc-4af7-880d-7e7d634a7833",
   "metadata": {},
   "source": [
    "# MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c404b841-2cf8-40c4-844c-aea086bf74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_c3.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bad9a5-c0db-4497-a6d5-db63521bb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.6027 \n",
      "Epoch 1: val_loss improved from inf to 0.66124, saving model to hasoc_c3.h5\n",
      "16/16 [==============================] - 228s 14s/step - loss: 0.6857 - accuracy: 0.6027 - val_loss: 0.6612 - val_accuracy: 0.6395\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 0.6388 - accuracy: 0.7606 \n",
      "Epoch 2: val_loss improved from 0.66124 to 0.64862, saving model to hasoc_c3.h5\n",
      "16/16 [==============================] - 224s 14s/step - loss: 0.6388 - accuracy: 0.7606 - val_loss: 0.6486 - val_accuracy: 0.6395\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.7833 \n",
      "Epoch 3: val_loss did not improve from 0.64862\n",
      "16/16 [==============================] - 224s 14s/step - loss: 0.5455 - accuracy: 0.7833 - val_loss: 0.6704 - val_accuracy: 0.6395\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.8359 \n",
      "Epoch 4: val_loss did not improve from 0.64862\n",
      "16/16 [==============================] - 223s 14s/step - loss: 0.4162 - accuracy: 0.8359 - val_loss: 0.6495 - val_accuracy: 0.6337\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2980 - accuracy: 0.8824 \n",
      "Epoch 5: val_loss did not improve from 0.64862\n",
      "16/16 [==============================] - 222s 14s/step - loss: 0.2980 - accuracy: 0.8824 - val_loss: 0.8180 - val_accuracy: 0.6105\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9071 \n",
      "Epoch 6: val_loss did not improve from 0.64862\n",
      "16/16 [==============================] - 225s 14s/step - loss: 0.2195 - accuracy: 0.9071 - val_loss: 0.9137 - val_accuracy: 0.6570\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9267 \n",
      "Epoch 7: val_loss did not improve from 0.64862\n",
      "16/16 [==============================] - 225s 14s/step - loss: 0.1732 - accuracy: 0.9267 - val_loss: 1.0342 - val_accuracy: 0.6628\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1396 - accuracy: 0.9432 \n",
      "Epoch 8: val_loss did not improve from 0.64862\n",
      "16/16 [==============================] - 226s 14s/step - loss: 0.1396 - accuracy: 0.9432 - val_loss: 1.2209 - val_accuracy: 0.6337\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9536 \n",
      "Epoch 9: val_loss did not improve from 0.64862\n",
      "16/16 [==============================] - 222s 14s/step - loss: 0.1259 - accuracy: 0.9536 - val_loss: 1.1681 - val_accuracy: 0.6395\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9598 \n",
      "Epoch 10: val_loss did not improve from 0.64862\n",
      "16/16 [==============================] - 227s 14s/step - loss: 0.1130 - accuracy: 0.9598 - val_loss: 1.4591 - val_accuracy: 0.6512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d385595460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim, input_length=2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# 假设Y_train已独热编码\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64, class_weight=class_weight_dict, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "838bc10e-c570-4b64-ab82-7e6c465d07da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 658ms/step - loss: 0.6486 - accuracy: 0.6395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6486157178878784, 0.6395348906517029]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_c3.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73e80567-a6a0-4901-827e-862bc494f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 17s 676ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f478079-754c-49aa-a518-74e7e531f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Vulgar':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ae99e70-20cb-4bb2-8bde-ead60f8c421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.71      0.80       708\n",
      "           1       0.06      0.21      0.09        61\n",
      "\n",
      "    accuracy                           0.67       769\n",
      "   macro avg       0.49      0.46      0.44       769\n",
      "weighted avg       0.84      0.67      0.74       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual, pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e809f0f4-ed4f-467c-ba37-3cff12268e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Vulgar')\n",
    "    else :\n",
    "        pred_actual.append('Non Vulgar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dccea22b-01c4-4632-ba38-1cfb802624b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id       label\n",
       "0   Hindi_image_410.jpg  Non Vulgar\n",
       "1   Hindi_image_114.jpg      Vulgar\n",
       "2   Hindi_image_101.jpg      Vulgar\n",
       "3  Hindi_image_1747.jpg  Non Vulgar\n",
       "4    Hindi_image_19.jpg      Vulgar"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_c3.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b7f46-5383-41b8-b966-fcc6f19027b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b92a9999-7083-4357-bf86-b352f73c6da6",
   "metadata": {},
   "source": [
    "# task_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137eb23d-4dd6-4788-b25a-a57f8873b0a6",
   "metadata": {},
   "source": [
    "### Handling Pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58522b77-cf5f-47c5-b6ae-d916d25012b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_1817.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Ba8@ DaNn G@rainiD IR T३ PDBB WRHE W PRD BCEN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_7.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Nari nari mat kar pagle, Nari he nark ka dwar....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_1.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>Kitni push ops maarsakte ho dafly? 5 aur agar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_32.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>अब इसमें मेरी कहां गलती है बताओ.. तरबूज़ वाली क...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_1714.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>\"KUDI MENU KEHNDl... 'MENU JUTI LA DE SONIYE.....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id         task_1      task_3   task_4  \\\n",
       "0  Hindi_image_1817.jpg      Sarcastic      Vulgar  Abusive   \n",
       "1     Hindi_image_7.jpg  Non-Sarcastic      Vulgar  Abusive   \n",
       "2     Hindi_image_1.jpg      Sarcastic  Non Vulgar  Abusive   \n",
       "3    Hindi_image_32.jpg      Sarcastic      Vulgar  Abusive   \n",
       "4  Hindi_image_1714.jpg      Sarcastic  Non Vulgar  Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Ba8@ DaNn G@rainiD IR T३ PDBB WRHE W PRD BCEN ...  \n",
       "1  Nari nari mat kar pagle, Nari he nark ka dwar....  \n",
       "2  Kitni push ops maarsakte ho dafly? 5 aur agar ...  \n",
       "3  अब इसमें मेरी कहां गलती है बताओ.. तरबूज़ वाली क...  \n",
       "4  \"KUDI MENU KEHNDl... 'MENU JUTI LA DE SONIYE.....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocess_data.csv')\n",
    "data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1c638b-1fa0-4024-a94d-71b5c6247f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>Sign You are Bancho a] _ ~\"11|7 have best ffen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>एक महिला घोडे़ के लिंग लिया| घोड़ा उत्साहित हो...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>एक टीचर ने एक लड़के को पेपर में नक़ल करते पकड लि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>show me Sckht Launda Kisslay Jha CTrollerlzabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>पति सुहागरात में पत्नी की निप्पल चूसते हुए बोल...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id         task_1      task_3       task_4  \\\n",
       "0   Hindi_image_410.jpg      Sarcastic      Vulgar  Non-abusive   \n",
       "1   Hindi_image_114.jpg  Non-Sarcastic      Vulgar      Abusive   \n",
       "2   Hindi_image_101.jpg  Non-Sarcastic  Non Vulgar  Non-abusive   \n",
       "3  Hindi_image_1747.jpg      Sarcastic      Vulgar      Abusive   \n",
       "4    Hindi_image_19.jpg  Non-Sarcastic  Non Vulgar      Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  Sign You are Bancho a] _ ~\"11|7 have best ffen...  \n",
       "1  एक महिला घोडे़ के लिंग लिया| घोड़ा उत्साहित हो...  \n",
       "2  एक टीचर ने एक लड़के को पेपर में नक़ल करते पकड लि...  \n",
       "3  show me Sckht Launda Kisslay Jha CTrollerlzabu...  \n",
       "4  पति सुहागरात में पत्नी की निप्पल चूसते हुए बोल...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../preprocess_test_data.csv') \n",
    "test_data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "test_data = test_data.drop(['Unnamed: 0'],axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5337489-c711-457f-bf15-08cccb214557",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text_clean'].astype(str)\n",
    "tokenizer = Tokenizer(num_words = 1500,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9611dd7f-3f9a-4434-8e48-358c28d051ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text_clean'].astype(str)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff17a6b-850d-4de7-bdd5-746380c02862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words :  8859\n",
      "[[   0    0    0 ... 1173 1174  571]\n",
      " [   0    0    0 ...   19   15 1176]\n",
      " [   0    0    0 ...    4  773   17]\n",
      " ...\n",
      " [   0    0    0 ...   36  377   30]\n",
      " [   0    0    0 ...   27  122  333]\n",
      " [   0    0    0 ...  118  739   89]]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 2500\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"No of unique words : \",len(index_of_words))\n",
    "\n",
    "X = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = data['task_4']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881882fe-1dc1-4f25-b3a7-3b31f718ee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   46  963    9]\n",
      " [   0    0    0 ...   45   35   26]\n",
      " [   0    0    0 ...  545  310    2]\n",
      " ...\n",
      " [   0    0    0 ...   87  143  333]\n",
      " [   0    0    0 ...    0    0  318]\n",
      " [   0    0    0 ...    0    0 1297]]\n"
     ]
    }
   ],
   "source": [
    "test_X = pad_sequences(test_sequence , maxlen = max_seq_len )\n",
    "test_Y = test_data['task_4']\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c1d9a80-ea66-45d7-ac81-932fe1683b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "vocabSize = len(index_of_words)\n",
    "lstm_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22796131-9412-43ef-9cdc-c8fdb2719700",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)\n",
    "Y_true = Y_test\n",
    "Y_train = pd.get_dummies(Y_train).values\n",
    "Y_test = pd.get_dummies(Y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe66d96-7075-4c81-a6ad-5a4b082a0e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Y: [[False  True]\n",
      " [ True False]\n",
      " [False  True]\n",
      " ...\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_true = test_Y\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "print(\"test_Y:\", test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae579fd8-92c3-4ed9-b8fc-bc8d43728109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6b986e-5edb-46b8-b0b2-d530868a2290",
   "metadata": {},
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26e858b2-521a-4c09-a93c-0bd6be0f50b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         2267904   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2350210 (8.97 MB)\n",
      "Trainable params: 2350210 (8.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim,input_length = 2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e35f2df6-c253-4457-8db0-219737bacf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_a4.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34044eaa-a11f-41ba-bd20-ac45083bb65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.6217 - accuracy: 0.7162\n",
      "Epoch 1: val_loss improved from inf to 0.54065, saving model to hasoc_a4.h5\n",
      "31/31 [==============================] - 130s 4s/step - loss: 0.6217 - accuracy: 0.7162 - val_loss: 0.5406 - val_accuracy: 0.7791\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 0.5630 - accuracy: 0.7214\n",
      "Epoch 2: val_loss improved from 0.54065 to 0.51541, saving model to hasoc_a4.h5\n",
      "31/31 [==============================] - 127s 4s/step - loss: 0.5630 - accuracy: 0.7214 - val_loss: 0.5154 - val_accuracy: 0.7791\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.7668\n",
      "Epoch 3: val_loss improved from 0.51541 to 0.48112, saving model to hasoc_a4.h5\n",
      "31/31 [==============================] - 132s 4s/step - loss: 0.4563 - accuracy: 0.7668 - val_loss: 0.4811 - val_accuracy: 0.7616\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.8700\n",
      "Epoch 4: val_loss did not improve from 0.48112\n",
      "31/31 [==============================] - 132s 4s/step - loss: 0.3059 - accuracy: 0.8700 - val_loss: 0.5192 - val_accuracy: 0.7442\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9216\n",
      "Epoch 5: val_loss did not improve from 0.48112\n",
      "31/31 [==============================] - 132s 4s/step - loss: 0.2063 - accuracy: 0.9216 - val_loss: 0.5894 - val_accuracy: 0.7442\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1435 - accuracy: 0.9401\n",
      "Epoch 6: val_loss did not improve from 0.48112\n",
      "31/31 [==============================] - 132s 4s/step - loss: 0.1435 - accuracy: 0.9401 - val_loss: 0.6462 - val_accuracy: 0.7151\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9525\n",
      "Epoch 7: val_loss did not improve from 0.48112\n",
      "31/31 [==============================] - 133s 4s/step - loss: 0.1198 - accuracy: 0.9525 - val_loss: 0.7894 - val_accuracy: 0.7733\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0932 - accuracy: 0.9608\n",
      "Epoch 8: val_loss did not improve from 0.48112\n",
      "31/31 [==============================] - 133s 4s/step - loss: 0.0932 - accuracy: 0.9608 - val_loss: 0.8310 - val_accuracy: 0.7442\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9608\n",
      "Epoch 9: val_loss did not improve from 0.48112\n",
      "31/31 [==============================] - 132s 4s/step - loss: 0.0778 - accuracy: 0.9608 - val_loss: 0.8987 - val_accuracy: 0.7326\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9577\n",
      "Epoch 10: val_loss did not improve from 0.48112\n",
      "31/31 [==============================] - 131s 4s/step - loss: 0.0750 - accuracy: 0.9577 - val_loss: 0.9657 - val_accuracy: 0.7442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d2073845e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train ,batch_size = 32, epochs = 10 ,validation_data=(X_test,Y_test) , callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "914986bc-021e-4206-b110-6e15d32e84a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 632ms/step - loss: 0.4811 - accuracy: 0.7616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48112398386001587, 0.7616279125213623]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_a4.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3616c6cb-c577-4615-a338-a57d8ec15854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 16s 620ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca45bf5a-db23-4ca3-be36-c58aefa60d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Non-abusive':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7692f2f9-9a64-409b-ae7c-dcc773270015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.14      0.08        21\n",
      "           1       0.97      0.94      0.96       748\n",
      "\n",
      "    accuracy                           0.92       769\n",
      "   macro avg       0.52      0.54      0.52       769\n",
      "weighted avg       0.95      0.92      0.93       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80800e71-0f5e-4d72-b8fd-ddce56d69f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [[0.09236269 0.9076373 ]\n",
      " [0.18006909 0.8199309 ]\n",
      " [0.20177443 0.7982255 ]\n",
      " ...\n",
      " [0.0238966  0.97610337]\n",
      " [0.351206   0.648794  ]\n",
      " [0.43851286 0.56148714]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_pred:\", Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bb05d7a-ccf8-4ca4-b3a5-23791865fbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_class: [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45c0aedf-7818-4be6-90a3-44df3fda6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Non-abusive')\n",
    "    else :\n",
    "        pred_actual.append('Abusive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd92004a-ed4c-4c10-92b0-0b5a806bb858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id        label\n",
       "0   Hindi_image_410.jpg  Non-abusive\n",
       "1   Hindi_image_114.jpg  Non-abusive\n",
       "2   Hindi_image_101.jpg  Non-abusive\n",
       "3  Hindi_image_1747.jpg  Non-abusive\n",
       "4    Hindi_image_19.jpg  Non-abusive"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_a4.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182019c-3f31-49b5-8f07-48962fd58cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4d7f23-0ba5-448b-a571-b6df5bfc91b7",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "049621db-c002-4e66-af9e-47425a6139f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         2267904   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2500, 256)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2350466 (8.97 MB)\n",
      "Trainable params: 2350338 (8.97 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding层，增加防止过拟合的Dropout\n",
    "model.add(Embedding(input_dim=vocabSize, output_dim=embed_dim, input_length=2500))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM层，增加recurrent_dropout 和 output_dropout\n",
    "model.add(LSTM(units=lstm_out, dropout=0.3, recurrent_dropout=0.3, return_sequences=False))\n",
    "\n",
    "# Batch Normalization增强泛化\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 全连接层，Softmax输出2分类，建议用categorical_crossentropy\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译\n",
    "optimizer = Adam(learning_rate=0.001)  # 学习率也可调整\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "095dd66d-c9e7-42fe-b28c-383cca29e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_b4.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a52e1185-0a85-4ba0-a884-878907a0c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.7346 - accuracy: 0.5243\n",
      "Epoch 1: val_loss improved from inf to 0.68081, saving model to hasoc_b4.h5\n",
      "31/31 [==============================] - 167s 5s/step - loss: 0.7346 - accuracy: 0.5243 - val_loss: 0.6808 - val_accuracy: 0.7326 - lr: 0.0010\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 0.4534 - accuracy: 0.7719\n",
      "Epoch 2: val_loss improved from 0.68081 to 0.60954, saving model to hasoc_b4.h5\n",
      "31/31 [==============================] - 165s 5s/step - loss: 0.4534 - accuracy: 0.7719 - val_loss: 0.6095 - val_accuracy: 0.7791 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8328\n",
      "Epoch 3: val_loss improved from 0.60954 to 0.55656, saving model to hasoc_b4.h5\n",
      "31/31 [==============================] - 165s 5s/step - loss: 0.3306 - accuracy: 0.8328 - val_loss: 0.5566 - val_accuracy: 0.7791 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.8772\n",
      "Epoch 4: val_loss improved from 0.55656 to 0.53398, saving model to hasoc_b4.h5\n",
      "31/31 [==============================] - 168s 5s/step - loss: 0.2511 - accuracy: 0.8772 - val_loss: 0.5340 - val_accuracy: 0.7733 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.2215 - accuracy: 0.9020\n",
      "Epoch 5: val_loss improved from 0.53398 to 0.51784, saving model to hasoc_b4.h5\n",
      "31/31 [==============================] - 167s 5s/step - loss: 0.2215 - accuracy: 0.9020 - val_loss: 0.5178 - val_accuracy: 0.7733 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9247\n",
      "Epoch 6: val_loss improved from 0.51784 to 0.51004, saving model to hasoc_b4.h5\n",
      "31/31 [==============================] - 165s 5s/step - loss: 0.1707 - accuracy: 0.9247 - val_loss: 0.5100 - val_accuracy: 0.7733 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9360\n",
      "Epoch 7: val_loss improved from 0.51004 to 0.49740, saving model to hasoc_b4.h5\n",
      "31/31 [==============================] - 162s 5s/step - loss: 0.1476 - accuracy: 0.9360 - val_loss: 0.4974 - val_accuracy: 0.7733 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9381\n",
      "Epoch 8: val_loss improved from 0.49740 to 0.49284, saving model to hasoc_b4.h5\n",
      "31/31 [==============================] - 162s 5s/step - loss: 0.1560 - accuracy: 0.9381 - val_loss: 0.4928 - val_accuracy: 0.7791 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.9422\n",
      "Epoch 9: val_loss did not improve from 0.49284\n",
      "31/31 [==============================] - 162s 5s/step - loss: 0.1305 - accuracy: 0.9422 - val_loss: 0.4936 - val_accuracy: 0.7849 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9360\n",
      "Epoch 10: val_loss did not improve from 0.49284\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "31/31 [==============================] - 162s 5s/step - loss: 0.1361 - accuracy: 0.9360 - val_loss: 0.4955 - val_accuracy: 0.7907 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d0eba98100>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# 假设Y_train已独热编码\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size = 32, class_weight=class_weight_dict, epochs = 10, validation_data=(X_test,Y_test), callbacks=[checkpoint, early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce578047-72a9-4c03-b677-7072424e3b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 653ms/step - loss: 0.4928 - accuracy: 0.7791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49283891916275024, 0.7790697813034058]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_b4.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04ade3bd-9017-4f20-8e8c-37ddf0dddd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 17s 681ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ebb1748-42f9-44c9-beb6-7e04ac0d5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Non-abusive':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd3acb7c-4665-4cac-a77c-1eff55760f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.05      0.06        21\n",
      "           1       0.97      0.98      0.98       748\n",
      "\n",
      "    accuracy                           0.96       769\n",
      "   macro avg       0.52      0.51      0.52       769\n",
      "weighted avg       0.95      0.96      0.95       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c18f7f5-0378-4e2b-85db-fba4563916dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Non-abusive')\n",
    "    else :\n",
    "        pred_actual.append('Abusive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6450fa65-5d3d-4795-b770-ac30316be509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id        label\n",
       "0   Hindi_image_410.jpg  Non-abusive\n",
       "1   Hindi_image_114.jpg  Non-abusive\n",
       "2   Hindi_image_101.jpg  Non-abusive\n",
       "3  Hindi_image_1747.jpg  Non-abusive\n",
       "4    Hindi_image_19.jpg  Non-abusive"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_b4.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b7950-ba1a-46ee-853c-75233f8a6262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "217ee86d-030b-4029-ba1b-1b754efbde64",
   "metadata": {},
   "source": [
    "# MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaf0caf7-e31a-4ae6-b004-fff80ee266dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_c4.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4abe340a-3b25-4e59-b2b8-be958d928ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5294 \n",
      "Epoch 1: val_loss improved from inf to 0.67930, saving model to hasoc_c4.h5\n",
      "16/16 [==============================] - 227s 14s/step - loss: 0.6912 - accuracy: 0.5294 - val_loss: 0.6793 - val_accuracy: 0.6686\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.6914 \n",
      "Epoch 2: val_loss improved from 0.67930 to 0.63836, saving model to hasoc_c4.h5\n",
      "16/16 [==============================] - 225s 14s/step - loss: 0.6561 - accuracy: 0.6914 - val_loss: 0.6384 - val_accuracy: 0.6628\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.5674 - accuracy: 0.7327 \n",
      "Epoch 3: val_loss improved from 0.63836 to 0.54868, saving model to hasoc_c4.h5\n",
      "16/16 [==============================] - 227s 14s/step - loss: 0.5674 - accuracy: 0.7327 - val_loss: 0.5487 - val_accuracy: 0.7093\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4092 - accuracy: 0.8431 \n",
      "Epoch 4: val_loss did not improve from 0.54868\n",
      "16/16 [==============================] - 222s 14s/step - loss: 0.4092 - accuracy: 0.8431 - val_loss: 0.5671 - val_accuracy: 0.6570\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.8906 \n",
      "Epoch 5: val_loss did not improve from 0.54868\n",
      "16/16 [==============================] - 225s 14s/step - loss: 0.2720 - accuracy: 0.8906 - val_loss: 0.6215 - val_accuracy: 0.7384\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9133 \n",
      "Epoch 6: val_loss did not improve from 0.54868\n",
      "16/16 [==============================] - 223s 14s/step - loss: 0.1925 - accuracy: 0.9133 - val_loss: 0.6853 - val_accuracy: 0.6977\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9154 \n",
      "Epoch 7: val_loss did not improve from 0.54868\n",
      "16/16 [==============================] - 226s 14s/step - loss: 0.1638 - accuracy: 0.9154 - val_loss: 0.7107 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9505 \n",
      "Epoch 8: val_loss did not improve from 0.54868\n",
      "16/16 [==============================] - 223s 14s/step - loss: 0.1287 - accuracy: 0.9505 - val_loss: 0.7936 - val_accuracy: 0.6977\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9391 \n",
      "Epoch 9: val_loss did not improve from 0.54868\n",
      "16/16 [==============================] - 224s 14s/step - loss: 0.1134 - accuracy: 0.9391 - val_loss: 0.8439 - val_accuracy: 0.7093\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9546 \n",
      "Epoch 10: val_loss did not improve from 0.54868\n",
      "16/16 [==============================] - 225s 14s/step - loss: 0.0964 - accuracy: 0.9546 - val_loss: 0.9324 - val_accuracy: 0.7267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x249b0a85880>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim, input_length=2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# 假设Y_train已独热编码\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64, class_weight=class_weight_dict, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c5c0e24-e30a-41f7-8149-b299632105f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 693ms/step - loss: 0.5487 - accuracy: 0.7093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5486761927604675, 0.7093023061752319]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_c4.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ce40eb-a452-4aec-8f98-923c3f00de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 19s 739ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f32d0dc-42eb-49fb-b747-e580910e43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Non-abusive':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec16f940-6826-410e-8a93-89d1545e97db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.29      0.04        21\n",
      "           1       0.97      0.67      0.79       748\n",
      "\n",
      "    accuracy                           0.66       769\n",
      "   macro avg       0.50      0.48      0.42       769\n",
      "weighted avg       0.94      0.66      0.77       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "139b3c35-f1a5-494f-a571-7922b0030f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Non-abusive')\n",
    "    else :\n",
    "        pred_actual.append('Abusive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48751ed7-d832-4272-a902-d6ac81ce4b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hindi_image_410.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi_image_114.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hindi_image_101.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi_image_1747.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi_image_19.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    _id        label\n",
       "0   Hindi_image_410.jpg  Non-abusive\n",
       "1   Hindi_image_114.jpg  Non-abusive\n",
       "2   Hindi_image_101.jpg  Non-abusive\n",
       "3  Hindi_image_1747.jpg  Non-abusive\n",
       "4    Hindi_image_19.jpg  Non-abusive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_c4.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120fffb-a2a3-490d-bf3d-d9066c0e62ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
