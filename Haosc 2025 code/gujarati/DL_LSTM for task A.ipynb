{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blond-share",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "democratic-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a63c8e-9446-4bc5-af61-589739e24b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc250e8-f5c9-40c1-977b-57a3cad6262a",
   "metadata": {},
   "source": [
    "# task_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-reynolds",
   "metadata": {},
   "source": [
    "### Handling Pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statutory-athens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1618.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકર). ટીચર તમાર તાજમહેલ\\r\\n\\r\\nદેખ/ય છે.\\r\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_31.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરો : ના.\\r\\n છોકરી : કેમ?\\r\\n \\r\\n છોકરી : ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1144.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરીઓ ગમે તેટલી\\r\\n ચાલક હોય,\\r\\n \\r\\n પણ છોક...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1184.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>દોસ્તી કરો,પ્રેમ કરો, વફા કરો...\\r\\n અને બહુ મ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1643.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરીઓ ગમે તેટલી\\r\\nચાલક હોય,\\r\\n\\r\\nપણ છોકરા ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id     task_1  task_3   task_4  \\\n",
       "0  Gujarati_image_1618.jpg  Sarcastic  Vulgar  Abusive   \n",
       "1    Gujarati_image_31.jpg  Sarcastic  Vulgar  Abusive   \n",
       "2  Gujarati_image_1144.jpg  Sarcastic  Vulgar  Abusive   \n",
       "3  Gujarati_image_1184.jpg  Sarcastic  Vulgar  Abusive   \n",
       "4  Gujarati_image_1643.jpg  Sarcastic  Vulgar  Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  છોકર). ટીચર તમાર તાજમહેલ\\r\\n\\r\\nદેખ/ય છે.\\r\\n\\...  \n",
       "1  છોકરો : ના.\\r\\n છોકરી : કેમ?\\r\\n \\r\\n છોકરી : ...  \n",
       "2  છોકરીઓ ગમે તેટલી\\r\\n ચાલક હોય,\\r\\n \\r\\n પણ છોક...  \n",
       "3  દોસ્તી કરો,પ્રેમ કરો, વફા કરો...\\r\\n અને બહુ મ...  \n",
       "4  છોકરીઓ ગમે તેટલી\\r\\nચાલક હોય,\\r\\n\\r\\nપણ છોકરા ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocess_data.csv')\n",
    "data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d27a324f-7d70-4f8c-8478-48cc2fb4708f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>॥વિંદેશીગામડિયો\\r\\n અ |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>ટીચર : સૌથી વધારે દુખાવો ક્યારે\\r\\nથાય?\\r\\nછોક...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>પતિ: તુંમને જરાય પ્રેમ\\r\\nનથી કરતી...\\r\\n\\r\\nપ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>આખા ગોમ ના લોડા\\r\\nભોસ મા ભરી ને બેઠી\\r\\nહોય અ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>મિનરલ વોટર સિવાય ક્યારેય\\r\\nબીજું\\r\\nપાણી નો પ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id         task_1      task_3       task_4  \\\n",
       "0  Gujarati_image_1225.jpg  Non-Sarcastic  Non Vulgar  Non-abusive   \n",
       "1  Gujarati_image_1583.jpg      Sarcastic      Vulgar      Abusive   \n",
       "2  Gujarati_image_1502.jpg      Sarcastic      Vulgar      Abusive   \n",
       "3  Gujarati_image_1487.jpg      Sarcastic      Vulgar      Abusive   \n",
       "4  Gujarati_image_1497.jpg  Non-Sarcastic      Vulgar      Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0                            ॥વિંદેશીગામડિયો\\r\\n અ |  \n",
       "1  ટીચર : સૌથી વધારે દુખાવો ક્યારે\\r\\nથાય?\\r\\nછોક...  \n",
       "2  પતિ: તુંમને જરાય પ્રેમ\\r\\nનથી કરતી...\\r\\n\\r\\nપ...  \n",
       "3  આખા ગોમ ના લોડા\\r\\nભોસ મા ભરી ને બેઠી\\r\\nહોય અ...  \n",
       "4  મિનરલ વોટર સિવાય ક્યારેય\\r\\nબીજું\\r\\nપાણી નો પ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../preprocess_test_data.csv') \n",
    "test_data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "test_data = test_data.drop(['Unnamed: 0'],axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "double-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text_clean'].astype(str)\n",
    "tokenizer = Tokenizer(num_words = 1500,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caabed00-ec76-4a8f-a01e-2766b6af9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text_clean'].astype(str)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "neither-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words :  7000\n",
      "[[   0    0    0 ...  536  134    1]\n",
      " [   0    0    0 ...  246 1062  318]\n",
      " [   0    0    0 ... 1067 1068  174]\n",
      " ...\n",
      " [   0    0    0 ...    0    0    5]\n",
      " [   0    0    0 ...    0    0    5]\n",
      " [   0    0    0 ...    1   17  433]]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 2500\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"No of unique words : \",len(index_of_words))\n",
    "\n",
    "X = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = data['task_1']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a2aa752-3621-4873-b884-59339d77d7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0 648]\n",
      " [  0   0   0 ... 206 394   1]\n",
      " [  0   0   0 ...   2   1   1]\n",
      " ...\n",
      " [  0   0   0 ...   1   1 324]\n",
      " [  0   0   0 ...  40   2   1]\n",
      " [  0   0   0 ...   0   0   5]]\n"
     ]
    }
   ],
   "source": [
    "test_X = pad_sequences(test_sequence , maxlen = max_seq_len )\n",
    "test_Y = test_data['task_1']\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proved-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "vocabSize = len(index_of_words)\n",
    "lstm_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "graduate-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)\n",
    "Y_true = Y_test\n",
    "Y_train = pd.get_dummies(Y_train).values\n",
    "Y_test = pd.get_dummies(Y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d933a8d7-7101-4330-8cef-694201627cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Y: [[ True False]\n",
      " [False  True]\n",
      " [False  True]\n",
      " ...\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_true = test_Y\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "print(\"test_Y:\", test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417287d3-96a2-4427-95b3-27d27e7732dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c520980b-1125-4977-ae1a-27331d6f2f9c",
   "metadata": {},
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compact-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         1792000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1874306 (7.15 MB)\n",
      "Trainable params: 1874306 (7.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim,input_length = 2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0060c2bd-d4a9-4b33-b121-c180a54af986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_a1.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d85b3f-d97a-4b14-8d1d-b7d79f3d8197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6109 - accuracy: 0.7351\n",
      "Epoch 1: val_loss improved from inf to 0.52641, saving model to hasoc_a1.h5\n",
      "24/24 [==============================] - 126s 5s/step - loss: 0.6109 - accuracy: 0.7351 - val_loss: 0.5264 - val_accuracy: 0.7836\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.7483\n",
      "Epoch 2: val_loss improved from 0.52641 to 0.50465, saving model to hasoc_a1.h5\n",
      "24/24 [==============================] - 124s 5s/step - loss: 0.5496 - accuracy: 0.7483 - val_loss: 0.5046 - val_accuracy: 0.7836\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.7576\n",
      "Epoch 3: val_loss improved from 0.50465 to 0.49123, saving model to hasoc_a1.h5\n",
      "24/24 [==============================] - 125s 5s/step - loss: 0.5002 - accuracy: 0.7576 - val_loss: 0.4912 - val_accuracy: 0.7910\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8159\n",
      "Epoch 4: val_loss did not improve from 0.49123\n",
      "24/24 [==============================] - 124s 5s/step - loss: 0.3910 - accuracy: 0.8159 - val_loss: 0.5532 - val_accuracy: 0.7687\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3012 - accuracy: 0.8967\n",
      "Epoch 5: val_loss did not improve from 0.49123\n",
      "24/24 [==============================] - 124s 5s/step - loss: 0.3012 - accuracy: 0.8967 - val_loss: 0.5617 - val_accuracy: 0.7687\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2258 - accuracy: 0.9192\n",
      "Epoch 6: val_loss did not improve from 0.49123\n",
      "24/24 [==============================] - 124s 5s/step - loss: 0.2258 - accuracy: 0.9192 - val_loss: 0.7177 - val_accuracy: 0.7388\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9272\n",
      "Epoch 7: val_loss did not improve from 0.49123\n",
      "24/24 [==============================] - 124s 5s/step - loss: 0.1851 - accuracy: 0.9272 - val_loss: 0.6890 - val_accuracy: 0.7388\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1729 - accuracy: 0.9325\n",
      "Epoch 8: val_loss did not improve from 0.49123\n",
      "24/24 [==============================] - 124s 5s/step - loss: 0.1729 - accuracy: 0.9325 - val_loss: 0.7795 - val_accuracy: 0.7313\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9325\n",
      "Epoch 9: val_loss did not improve from 0.49123\n",
      "24/24 [==============================] - 125s 5s/step - loss: 0.1641 - accuracy: 0.9325 - val_loss: 0.7682 - val_accuracy: 0.7388\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9377\n",
      "Epoch 10: val_loss did not improve from 0.49123\n",
      "24/24 [==============================] - 126s 5s/step - loss: 0.1618 - accuracy: 0.9377 - val_loss: 0.7752 - val_accuracy: 0.7090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23751342c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75492eb6-7439-4cab-9251-c10b526752d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 525ms/step - loss: 0.4912 - accuracy: 0.7910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49123457074165344, 0.7910447716712952]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_a1.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80506f73-41f9-4a1d-9ebf-d0fe0451facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 12s 610ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ed8c82-d08a-4312-9675-642375927eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Sarcastic':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83ec6e38-4c02-416b-9695-560f0d6c23ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.00      0.01       410\n",
      "           1       0.32      0.98      0.48       194\n",
      "\n",
      "    accuracy                           0.32       604\n",
      "   macro avg       0.33      0.49      0.24       604\n",
      "weighted avg       0.33      0.32      0.16       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual, pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f41268e-489f-45bb-9a3a-a01bd2cd9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [[0.10724969 0.8927503 ]\n",
      " [0.35344046 0.64655954]\n",
      " [0.03545646 0.9645435 ]\n",
      " ...\n",
      " [0.08703656 0.9129634 ]\n",
      " [0.09556682 0.9044332 ]\n",
      " [0.13938175 0.86061823]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_pred:\", Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9efa457b-540d-46de-a115-211be3a7d33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_class: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf06adfc-cc6f-43b3-acb2-effb9a6b4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Sarcastic')\n",
    "    else :\n",
    "        pred_actual.append('Non-Sarcastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5b90d1-c2be-44f8-8743-431e6f0ebaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id      label\n",
       "0  Gujarati_image_1225.jpg  Sarcastic\n",
       "1  Gujarati_image_1583.jpg  Sarcastic\n",
       "2  Gujarati_image_1502.jpg  Sarcastic\n",
       "3  Gujarati_image_1487.jpg  Sarcastic\n",
       "4  Gujarati_image_1497.jpg  Sarcastic"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_a.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c129ed3-ab34-45fe-ad37-8669648e0354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e4b66b1-bb08-4215-ada1-b4464865758a",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc20ae1-961d-4ac2-8dba-045c24a75e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         1792000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2500, 256)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1874562 (7.15 MB)\n",
      "Trainable params: 1874434 (7.15 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding层，增加防止过拟合的Dropout\n",
    "model.add(Embedding(input_dim=vocabSize, output_dim=embed_dim, input_length=2500))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM层，增加recurrent_dropout 和 output dropout\n",
    "model.add(LSTM(units=lstm_out, dropout=0.3, recurrent_dropout=0.3, return_sequences=False))\n",
    "\n",
    "# Batch Normalization增强泛化\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 全连接层，Softmax输出2分类，建议用categorical_crossentropy\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译\n",
    "optimizer = Adam(learning_rate=0.001)  # 学习率也可调整\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "universal-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"hasoc_b1.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
    "save_weights_only=False, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fifteen-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7247 - accuracy: 0.5113\n",
      "Epoch 1: val_loss improved from inf to 0.60993, saving model to hasoc_b1.h5\n",
      "24/24 [==============================] - 125s 5s/step - loss: 0.7247 - accuracy: 0.5113 - val_loss: 0.6099 - val_accuracy: 0.7836\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.7642\n",
      "Epoch 2: val_loss improved from 0.60993 to 0.56328, saving model to hasoc_b1.h5\n",
      "24/24 [==============================] - 120s 5s/step - loss: 0.5304 - accuracy: 0.7642 - val_loss: 0.5633 - val_accuracy: 0.7836\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4138 - accuracy: 0.8371\n",
      "Epoch 3: val_loss improved from 0.56328 to 0.53399, saving model to hasoc_b1.h5\n",
      "24/24 [==============================] - 122s 5s/step - loss: 0.4138 - accuracy: 0.8371 - val_loss: 0.5340 - val_accuracy: 0.7836\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.8583\n",
      "Epoch 4: val_loss improved from 0.53399 to 0.51653, saving model to hasoc_b1.h5\n",
      "24/24 [==============================] - 123s 5s/step - loss: 0.3294 - accuracy: 0.8583 - val_loss: 0.5165 - val_accuracy: 0.7836\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2735 - accuracy: 0.8795\n",
      "Epoch 5: val_loss improved from 0.51653 to 0.51339, saving model to hasoc_b1.h5\n",
      "24/24 [==============================] - 122s 5s/step - loss: 0.2735 - accuracy: 0.8795 - val_loss: 0.5134 - val_accuracy: 0.7836\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9060\n",
      "Epoch 6: val_loss did not improve from 0.51339\n",
      "24/24 [==============================] - 122s 5s/step - loss: 0.2440 - accuracy: 0.9060 - val_loss: 0.5181 - val_accuracy: 0.7836\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.9126\n",
      "Epoch 7: val_loss did not improve from 0.51339\n",
      "24/24 [==============================] - 123s 5s/step - loss: 0.2056 - accuracy: 0.9126 - val_loss: 0.5186 - val_accuracy: 0.7836\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9126\n",
      "Epoch 8: val_loss did not improve from 0.51339\n",
      "24/24 [==============================] - 123s 5s/step - loss: 0.2069 - accuracy: 0.9126 - val_loss: 0.5337 - val_accuracy: 0.7836\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9258\n",
      "Epoch 9: val_loss did not improve from 0.51339\n",
      "24/24 [==============================] - 123s 5s/step - loss: 0.1995 - accuracy: 0.9258 - val_loss: 0.5234 - val_accuracy: 0.7836\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9338\n",
      "Epoch 10: val_loss did not improve from 0.51339\n",
      "24/24 [==============================] - 122s 5s/step - loss: 0.1629 - accuracy: 0.9338 - val_loss: 0.5205 - val_accuracy: 0.7836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b1ad28ceb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, validation_data = (X_test,Y_test), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9ffe36-7cd2-497e-a333-9e12d83d4f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 572ms/step - loss: 0.5134 - accuracy: 0.7836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.513394296169281, 0.7835820913314819]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_b1.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4915eb9-b352-47a9-b199-b1fbb78e031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 11s 596ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a1205a-9155-4825-a81c-70e62cad51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Sarcastic':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56f4a6d7-1137-48e9-8581-5cfc31dd4f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       410\n",
      "           1       0.32      1.00      0.49       194\n",
      "\n",
      "    accuracy                           0.32       604\n",
      "   macro avg       0.16      0.50      0.24       604\n",
      "weighted avg       0.10      0.32      0.16       604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb60dd13-75bb-4347-a0df-751c4397fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Sarcastic')\n",
    "    else :\n",
    "        pred_actual.append('Non-Sarcastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26bd3d87-2719-4b05-a43f-a7b772760251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id      label\n",
       "0  Gujarati_image_1225.jpg  Sarcastic\n",
       "1  Gujarati_image_1583.jpg  Sarcastic\n",
       "2  Gujarati_image_1502.jpg  Sarcastic\n",
       "3  Gujarati_image_1487.jpg  Sarcastic\n",
       "4  Gujarati_image_1497.jpg  Sarcastic"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_b.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360aa228-e6c4-47a8-bff1-d281f4603672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5e794f2-1ade-4f3e-88d1-9d21984b4be2",
   "metadata": {},
   "source": [
    "# MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056e1b18-4d02-4f86-acbc-88e65d43f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_c1.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe56d8e-a2ea-4275-8c7f-03954703df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.4768 \n",
      "Epoch 1: val_loss improved from inf to 0.67589, saving model to hasoc_c1.h5\n",
      "12/12 [==============================] - 193s 16s/step - loss: 0.6941 - accuracy: 0.4768 - val_loss: 0.6759 - val_accuracy: 0.7015\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - ETA: 0s - loss: 0.6738 - accuracy: 0.7497 \n",
      "Epoch 2: val_loss improved from 0.67589 to 0.65912, saving model to hasoc_c1.h5\n",
      "12/12 [==============================] - 188s 16s/step - loss: 0.6738 - accuracy: 0.7497 - val_loss: 0.6591 - val_accuracy: 0.7164\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.7020 \n",
      "Epoch 3: val_loss improved from 0.65912 to 0.56992, saving model to hasoc_c1.h5\n",
      "12/12 [==============================] - 201s 17s/step - loss: 0.6313 - accuracy: 0.7020 - val_loss: 0.5699 - val_accuracy: 0.8060\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.8000 \n",
      "Epoch 4: val_loss did not improve from 0.56992\n",
      "12/12 [==============================] - 199s 17s/step - loss: 0.5243 - accuracy: 0.8000 - val_loss: 0.6379 - val_accuracy: 0.5970\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3710 - accuracy: 0.8159 \n",
      "Epoch 5: val_loss did not improve from 0.56992\n",
      "12/12 [==============================] - 196s 16s/step - loss: 0.3710 - accuracy: 0.8159 - val_loss: 0.5970 - val_accuracy: 0.7239\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.8305 \n",
      "Epoch 6: val_loss did not improve from 0.56992\n",
      "12/12 [==============================] - 198s 16s/step - loss: 0.2940 - accuracy: 0.8305 - val_loss: 0.6023 - val_accuracy: 0.6493\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.8583 \n",
      "Epoch 7: val_loss did not improve from 0.56992\n",
      "12/12 [==============================] - 196s 16s/step - loss: 0.2569 - accuracy: 0.8583 - val_loss: 0.7140 - val_accuracy: 0.6418\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.8371 \n",
      "Epoch 8: val_loss did not improve from 0.56992\n",
      "12/12 [==============================] - 192s 16s/step - loss: 0.2396 - accuracy: 0.8371 - val_loss: 0.6698 - val_accuracy: 0.7463\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.8887 \n",
      "Epoch 9: val_loss did not improve from 0.56992\n",
      "12/12 [==============================] - 167s 14s/step - loss: 0.2142 - accuracy: 0.8887 - val_loss: 0.8029 - val_accuracy: 0.6716\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.8901 \n",
      "Epoch 10: val_loss did not improve from 0.56992\n",
      "12/12 [==============================] - 167s 14s/step - loss: 0.1985 - accuracy: 0.8901 - val_loss: 0.8455 - val_accuracy: 0.6493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x193e2d35880>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim, input_length=2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# 假设Y_train已独热编码\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64, class_weight=class_weight_dict, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conceptual-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 649ms/step - loss: 0.5699 - accuracy: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5699222087860107, 0.8059701323509216]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_c1.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "facial-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 14s 739ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "right-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Sarcastic':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "anticipated-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.05      0.10       410\n",
      "           1       0.32      0.94      0.48       194\n",
      "\n",
      "    accuracy                           0.34       604\n",
      "   macro avg       0.49      0.50      0.29       604\n",
      "weighted avg       0.55      0.34      0.22       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "combined-reduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [[0.4008175  0.5991824 ]\n",
      " [0.5229386  0.47706142]\n",
      " [0.32581317 0.6741869 ]\n",
      " ...\n",
      " [0.36569163 0.6343084 ]\n",
      " [0.4074059  0.5925941 ]\n",
      " [0.44256213 0.55743796]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_pred:\",Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "large-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Sarcastic')\n",
    "    else :\n",
    "        pred_actual.append('Non-Sarcastic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf19092-56d9-4e1d-becc-4ba884cae376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id          label\n",
       "0  Gujarati_image_1225.jpg      Sarcastic\n",
       "1  Gujarati_image_1583.jpg  Non-Sarcastic\n",
       "2  Gujarati_image_1502.jpg      Sarcastic\n",
       "3  Gujarati_image_1487.jpg      Sarcastic\n",
       "4  Gujarati_image_1497.jpg  Non-Sarcastic"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_c.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39eebbf-f926-457d-9559-71e0aaa3df54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04f3adc6-faf7-4aea-89e5-4b2ca5566698",
   "metadata": {},
   "source": [
    "# task_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f882fe-2c95-4e19-829f-d48c66d73319",
   "metadata": {},
   "source": [
    "# Handling Pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902d04ee-2ca5-4208-968d-a01b98ae5e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1618.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકર). ટીચર તમાર તાજમહેલ\\r\\n\\r\\nદેખ/ય છે.\\r\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_31.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરો : ના.\\r\\n છોકરી : કેમ?\\r\\n \\r\\n છોકરી : ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1144.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરીઓ ગમે તેટલી\\r\\n ચાલક હોય,\\r\\n \\r\\n પણ છોક...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1184.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>દોસ્તી કરો,પ્રેમ કરો, વફા કરો...\\r\\n અને બહુ મ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1643.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરીઓ ગમે તેટલી\\r\\nચાલક હોય,\\r\\n\\r\\nપણ છોકરા ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id     task_1  task_3   task_4  \\\n",
       "0  Gujarati_image_1618.jpg  Sarcastic  Vulgar  Abusive   \n",
       "1    Gujarati_image_31.jpg  Sarcastic  Vulgar  Abusive   \n",
       "2  Gujarati_image_1144.jpg  Sarcastic  Vulgar  Abusive   \n",
       "3  Gujarati_image_1184.jpg  Sarcastic  Vulgar  Abusive   \n",
       "4  Gujarati_image_1643.jpg  Sarcastic  Vulgar  Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  છોકર). ટીચર તમાર તાજમહેલ\\r\\n\\r\\nદેખ/ય છે.\\r\\n\\...  \n",
       "1  છોકરો : ના.\\r\\n છોકરી : કેમ?\\r\\n \\r\\n છોકરી : ...  \n",
       "2  છોકરીઓ ગમે તેટલી\\r\\n ચાલક હોય,\\r\\n \\r\\n પણ છોક...  \n",
       "3  દોસ્તી કરો,પ્રેમ કરો, વફા કરો...\\r\\n અને બહુ મ...  \n",
       "4  છોકરીઓ ગમે તેટલી\\r\\nચાલક હોય,\\r\\n\\r\\nપણ છોકરા ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocess_data.csv')\n",
    "data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d86215-a94a-42c3-8e44-2cbe52de7441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>॥વિંદેશીગામડિયો\\r\\n અ |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>ટીચર : સૌથી વધારે દુખાવો ક્યારે\\r\\nથાય?\\r\\nછોક...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>પતિ: તુંમને જરાય પ્રેમ\\r\\nનથી કરતી...\\r\\n\\r\\nપ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>આખા ગોમ ના લોડા\\r\\nભોસ મા ભરી ને બેઠી\\r\\nહોય અ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>મિનરલ વોટર સિવાય ક્યારેય\\r\\nબીજું\\r\\nપાણી નો પ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id         task_1      task_3       task_4  \\\n",
       "0  Gujarati_image_1225.jpg  Non-Sarcastic  Non Vulgar  Non-abusive   \n",
       "1  Gujarati_image_1583.jpg      Sarcastic      Vulgar      Abusive   \n",
       "2  Gujarati_image_1502.jpg      Sarcastic      Vulgar      Abusive   \n",
       "3  Gujarati_image_1487.jpg      Sarcastic      Vulgar      Abusive   \n",
       "4  Gujarati_image_1497.jpg  Non-Sarcastic      Vulgar      Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0                            ॥વિંદેશીગામડિયો\\r\\n અ |  \n",
       "1  ટીચર : સૌથી વધારે દુખાવો ક્યારે\\r\\nથાય?\\r\\nછોક...  \n",
       "2  પતિ: તુંમને જરાય પ્રેમ\\r\\nનથી કરતી...\\r\\n\\r\\nપ...  \n",
       "3  આખા ગોમ ના લોડા\\r\\nભોસ મા ભરી ને બેઠી\\r\\nહોય અ...  \n",
       "4  મિનરલ વોટર સિવાય ક્યારેય\\r\\nબીજું\\r\\nપાણી નો પ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../preprocess_test_data.csv') \n",
    "test_data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "test_data = test_data.drop(['Unnamed: 0'],axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b76f150-a989-4c23-be65-555d7b8ce4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text_clean'].astype(str)\n",
    "tokenizer = Tokenizer(num_words = 1500,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0db82e-c39e-48a3-a1ce-b4f678943151",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text_clean'].astype(str)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c60aee9-ac08-46b7-920e-95c44671b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words :  7000\n",
      "[[   0    0    0 ...  536  134    1]\n",
      " [   0    0    0 ...  246 1062  318]\n",
      " [   0    0    0 ... 1067 1068  174]\n",
      " ...\n",
      " [   0    0    0 ...    0    0    5]\n",
      " [   0    0    0 ...    0    0    5]\n",
      " [   0    0    0 ...    1   17  433]]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 2500\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"No of unique words : \",len(index_of_words))\n",
    "\n",
    "X = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = data['task_3']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4278bf1-abdc-4bad-a741-9009e8fd76cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0 648]\n",
      " [  0   0   0 ... 206 394   1]\n",
      " [  0   0   0 ...   2   1   1]\n",
      " ...\n",
      " [  0   0   0 ...   1   1 324]\n",
      " [  0   0   0 ...  40   2   1]\n",
      " [  0   0   0 ...   0   0   5]]\n"
     ]
    }
   ],
   "source": [
    "test_X = pad_sequences(test_sequence , maxlen = max_seq_len )\n",
    "test_Y = test_data['task_3']\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c073a678-4d76-4072-bfd0-cc309c3a34ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "vocabSize = len(index_of_words)\n",
    "lstm_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b68e04d-4d28-4e59-b52b-69a7ac080e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)\n",
    "Y_true = Y_test\n",
    "Y_train = pd.get_dummies(Y_train).values\n",
    "Y_test = pd.get_dummies(Y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c87ebdca-b479-45ea-9f48-62bfd757767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Y: [[ True False]\n",
      " [False  True]\n",
      " [False  True]\n",
      " ...\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_true = test_Y\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "print(\"test_Y:\",test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dddc6f-e30f-4bed-8532-96039131f365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89d9fe63-44a2-42e4-922e-566c4fca9246",
   "metadata": {},
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0fcbb9-ee12-4468-ae04-a3761b23fa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         1792000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1874306 (7.15 MB)\n",
      "Trainable params: 1874306 (7.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim,input_length = 2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5ae185-28aa-45cf-8f11-b829df0d4518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_a3.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "719b8baf-abbb-4770-a717-c5a53d0be6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6070 - accuracy: 0.6596\n",
      "Epoch 1: val_loss improved from inf to 0.56218, saving model to hasoc_a3.h5\n",
      "24/24 [==============================] - 98s 4s/step - loss: 0.6070 - accuracy: 0.6596 - val_loss: 0.5622 - val_accuracy: 0.6567\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.7589\n",
      "Epoch 2: val_loss improved from 0.56218 to 0.52034, saving model to hasoc_a3.h5\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.5493 - accuracy: 0.7589 - val_loss: 0.5203 - val_accuracy: 0.8060\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.8358\n",
      "Epoch 3: val_loss improved from 0.52034 to 0.46331, saving model to hasoc_a3.h5\n",
      "24/24 [==============================] - 96s 4s/step - loss: 0.4582 - accuracy: 0.8358 - val_loss: 0.4633 - val_accuracy: 0.7985\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.8861\n",
      "Epoch 4: val_loss did not improve from 0.46331\n",
      "24/24 [==============================] - 96s 4s/step - loss: 0.3104 - accuracy: 0.8861 - val_loss: 0.5014 - val_accuracy: 0.7687\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9325\n",
      "Epoch 5: val_loss did not improve from 0.46331\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.2103 - accuracy: 0.9325 - val_loss: 0.6803 - val_accuracy: 0.7537\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9523\n",
      "Epoch 6: val_loss did not improve from 0.46331\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.1660 - accuracy: 0.9523 - val_loss: 0.6842 - val_accuracy: 0.7313\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.9576\n",
      "Epoch 7: val_loss did not improve from 0.46331\n",
      "24/24 [==============================] - 96s 4s/step - loss: 0.1345 - accuracy: 0.9576 - val_loss: 0.8065 - val_accuracy: 0.7463\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9589\n",
      "Epoch 8: val_loss did not improve from 0.46331\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.1281 - accuracy: 0.9589 - val_loss: 0.8137 - val_accuracy: 0.7537\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1178 - accuracy: 0.9629\n",
      "Epoch 9: val_loss did not improve from 0.46331\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.1178 - accuracy: 0.9629 - val_loss: 0.8887 - val_accuracy: 0.7313\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9629\n",
      "Epoch 10: val_loss did not improve from 0.46331\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.1092 - accuracy: 0.9629 - val_loss: 0.9863 - val_accuracy: 0.7313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23f09475a30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, validation_data = (X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e2aeab-52a3-4b24-b6c4-cf68da013c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 443ms/step - loss: 0.4633 - accuracy: 0.7985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4633116126060486, 0.7985074520111084]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_a3.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d81400a8-045f-4a08-bb39-22a51732d97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 10s 506ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31c36c37-ba1a-4f01-a9b1-edf06f1798da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i =='Vulgar':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ebb8b99-5433-4e84-8564-6ae5e603c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86       496\n",
      "           1       0.41      0.54      0.46       108\n",
      "\n",
      "    accuracy                           0.78       604\n",
      "   macro avg       0.65      0.68      0.66       604\n",
      "weighted avg       0.80      0.78      0.79       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual, pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1933064f-ac64-40eb-bf93-4befae2fbd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [[0.9903398  0.00966018]\n",
      " [0.04106864 0.9589314 ]\n",
      " [0.6246746  0.3753254 ]\n",
      " ...\n",
      " [0.96826684 0.0317331 ]\n",
      " [0.961996   0.03800403]\n",
      " [0.9824645  0.01753556]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_pred:\", Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2b4b1f2-a4b2-4791-8b03-bd7289cdf411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_class: [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0217bbe1-a400-4d25-801e-839e2881ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Vulgar')\n",
    "    else :\n",
    "        pred_actual.append('Non Vulgar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "903cf42d-ed9b-4882-8ba4-67b792528db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id       label\n",
       "0  Gujarati_image_1225.jpg  Non Vulgar\n",
       "1  Gujarati_image_1583.jpg      Vulgar\n",
       "2  Gujarati_image_1502.jpg  Non Vulgar\n",
       "3  Gujarati_image_1487.jpg  Non Vulgar\n",
       "4  Gujarati_image_1497.jpg  Non Vulgar"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_a3.csv', index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a497f-750c-4f4b-99f3-ce8bfca9a38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de25dbd4-615a-44ab-a315-833386a806d2",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "711950b3-4e93-4d23-af0f-2d20ab4ede92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 2500, 256)         1792000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2500, 256)         0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                82176     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1874562 (7.15 MB)\n",
      "Trainable params: 1874434 (7.15 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding层，增加防止过拟合的Dropout\n",
    "model.add(Embedding(input_dim=vocabSize, output_dim=embed_dim, input_length=2500))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM层，增加recurrent_dropout 和 output dropout\n",
    "model.add(LSTM(units=lstm_out, dropout=0.3, recurrent_dropout=0.3, return_sequences=False))\n",
    "\n",
    "# Batch Normalization增强泛化\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 全连接层，Softmax输出2分类，建议用categorical_crossentropy\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译\n",
    "optimizer = Adam(learning_rate=0.001)  # 学习率也可调整\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bde9e4c4-874e-439e-a59e-aece320bed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_b3.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc0f0198-3ed9-4f1b-9515-1351e3252bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6636 - accuracy: 0.6172\n",
      "Epoch 1: val_loss improved from inf to 0.64467, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 104s 4s/step - loss: 0.6636 - accuracy: 0.6172 - val_loss: 0.6447 - val_accuracy: 0.6567\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.7894\n",
      "Epoch 2: val_loss improved from 0.64467 to 0.62699, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 101s 4s/step - loss: 0.4786 - accuracy: 0.7894 - val_loss: 0.6270 - val_accuracy: 0.6567\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3241 - accuracy: 0.8623\n",
      "Epoch 3: val_loss improved from 0.62699 to 0.61049, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 101s 4s/step - loss: 0.3241 - accuracy: 0.8623 - val_loss: 0.6105 - val_accuracy: 0.6567\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9232\n",
      "Epoch 4: val_loss improved from 0.61049 to 0.60861, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 103s 4s/step - loss: 0.2255 - accuracy: 0.9232 - val_loss: 0.6086 - val_accuracy: 0.6567\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1946 - accuracy: 0.9325\n",
      "Epoch 5: val_loss improved from 0.60861 to 0.59023, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 103s 4s/step - loss: 0.1946 - accuracy: 0.9325 - val_loss: 0.5902 - val_accuracy: 0.6716\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1846 - accuracy: 0.9351\n",
      "Epoch 6: val_loss improved from 0.59023 to 0.58422, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 103s 4s/step - loss: 0.1846 - accuracy: 0.9351 - val_loss: 0.5842 - val_accuracy: 0.6866\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9258\n",
      "Epoch 7: val_loss improved from 0.58422 to 0.54906, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 104s 4s/step - loss: 0.1890 - accuracy: 0.9258 - val_loss: 0.5491 - val_accuracy: 0.7239\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1812 - accuracy: 0.9483\n",
      "Epoch 8: val_loss improved from 0.54906 to 0.53040, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 103s 4s/step - loss: 0.1812 - accuracy: 0.9483 - val_loss: 0.5304 - val_accuracy: 0.7388\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9589\n",
      "Epoch 9: val_loss did not improve from 0.53040\n",
      "24/24 [==============================] - 103s 4s/step - loss: 0.1383 - accuracy: 0.9589 - val_loss: 0.5543 - val_accuracy: 0.7239\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9536\n",
      "Epoch 10: val_loss improved from 0.53040 to 0.52165, saving model to hasoc_b3.h5\n",
      "24/24 [==============================] - 103s 4s/step - loss: 0.1414 - accuracy: 0.9536 - val_loss: 0.5216 - val_accuracy: 0.7388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23f09c1fb50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, validation_data = (X_test,Y_test), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41c00e01-ae9f-4e4c-b403-5c8fec3543e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 538ms/step - loss: 0.5216 - accuracy: 0.7388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.52164626121521, 0.7388059496879578]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_b3.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "180d0471-2322-44e1-8a9a-e2a3aab01def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 12s 609ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b77e8a8-a180-4441-bc33-5cd24b7f9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Vulgar':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58380c31-507e-4f30-b9e3-962ce1c44b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89       496\n",
      "           1       0.48      0.36      0.41       108\n",
      "\n",
      "    accuracy                           0.81       604\n",
      "   macro avg       0.67      0.64      0.65       604\n",
      "weighted avg       0.80      0.81      0.80       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual, pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fab9c471-1c05-4e7c-83af-afac9502c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Vulgar')\n",
    "    else :\n",
    "        pred_actual.append('Non Vulgar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c87f00fb-595f-4419-9aac-7da0e529ca3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id       label\n",
       "0  Gujarati_image_1225.jpg  Non Vulgar\n",
       "1  Gujarati_image_1583.jpg      Vulgar\n",
       "2  Gujarati_image_1502.jpg  Non Vulgar\n",
       "3  Gujarati_image_1487.jpg  Non Vulgar\n",
       "4  Gujarati_image_1497.jpg  Non Vulgar"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_b3.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a8754-5f01-498a-b6b4-fd2644062993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60282e11-4c27-415b-a2c1-89398685b1fe",
   "metadata": {},
   "source": [
    "# MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f04d8677-7964-46f0-bc2f-3f17c1ff9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_c3.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2204d263-b727-4d7a-a0b2-d1ccb2ea3ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6710 - accuracy: 0.6411 \n",
      "Epoch 1: val_loss improved from inf to 0.59976, saving model to hasoc_c3.h5\n",
      "12/12 [==============================] - 168s 14s/step - loss: 0.6710 - accuracy: 0.6411 - val_loss: 0.5998 - val_accuracy: 0.7164\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - ETA: 0s - loss: 0.6383 - accuracy: 0.6821 \n",
      "Epoch 2: val_loss did not improve from 0.59976\n",
      "12/12 [==============================] - 168s 14s/step - loss: 0.6383 - accuracy: 0.6821 - val_loss: 0.6054 - val_accuracy: 0.6567\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5828 - accuracy: 0.6702 \n",
      "Epoch 3: val_loss improved from 0.59976 to 0.59414, saving model to hasoc_c3.h5\n",
      "12/12 [==============================] - 165s 14s/step - loss: 0.5828 - accuracy: 0.6702 - val_loss: 0.5941 - val_accuracy: 0.6866\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5086 - accuracy: 0.7748 \n",
      "Epoch 4: val_loss improved from 0.59414 to 0.50209, saving model to hasoc_c3.h5\n",
      "12/12 [==============================] - 166s 14s/step - loss: 0.5086 - accuracy: 0.7748 - val_loss: 0.5021 - val_accuracy: 0.7985\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.8556 \n",
      "Epoch 5: val_loss did not improve from 0.50209\n",
      "12/12 [==============================] - 166s 14s/step - loss: 0.3924 - accuracy: 0.8556 - val_loss: 0.5023 - val_accuracy: 0.7985\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.9192 \n",
      "Epoch 6: val_loss did not improve from 0.50209\n",
      "12/12 [==============================] - 166s 14s/step - loss: 0.2713 - accuracy: 0.9192 - val_loss: 0.6161 - val_accuracy: 0.7537\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9338 \n",
      "Epoch 7: val_loss did not improve from 0.50209\n",
      "12/12 [==============================] - 170s 14s/step - loss: 0.2076 - accuracy: 0.9338 - val_loss: 0.6134 - val_accuracy: 0.7388\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9576 \n",
      "Epoch 8: val_loss did not improve from 0.50209\n",
      "12/12 [==============================] - 168s 14s/step - loss: 0.1657 - accuracy: 0.9576 - val_loss: 0.7602 - val_accuracy: 0.7687\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9497 \n",
      "Epoch 9: val_loss did not improve from 0.50209\n",
      "12/12 [==============================] - 166s 14s/step - loss: 0.1513 - accuracy: 0.9497 - val_loss: 0.7971 - val_accuracy: 0.7090\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9550 \n",
      "Epoch 10: val_loss did not improve from 0.50209\n",
      "12/12 [==============================] - 167s 14s/step - loss: 0.1432 - accuracy: 0.9550 - val_loss: 0.7644 - val_accuracy: 0.7313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f80ba34490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim, input_length=2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# 假设Y_train已独热编码\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64, class_weight=class_weight_dict, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1ae335-ebcb-472b-885c-4f75f7caf06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 683ms/step - loss: 0.5021 - accuracy: 0.7985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5020859241485596, 0.7985074520111084]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_c3.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db149554-fa48-4f43-aec3-881ef2c4e96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 14s 752ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "603b765d-86f2-46d1-bf86-ae26e1492236",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Vulgar':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4412358-a8fb-4b94-ae1c-c779ca704f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.67      0.79       496\n",
      "           1       0.35      0.81      0.49       108\n",
      "\n",
      "    accuracy                           0.70       604\n",
      "   macro avg       0.65      0.74      0.64       604\n",
      "weighted avg       0.84      0.70      0.73       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual, pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ba9008f-56cf-4bec-a60f-9f207591796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Vulgar')\n",
    "    else :\n",
    "        pred_actual.append('Non Vulgar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c07fc0f2-853c-4114-b92f-a63208645b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Vulgar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id       label\n",
       "0  Gujarati_image_1225.jpg  Non Vulgar\n",
       "1  Gujarati_image_1583.jpg      Vulgar\n",
       "2  Gujarati_image_1502.jpg      Vulgar\n",
       "3  Gujarati_image_1487.jpg      Vulgar\n",
       "4  Gujarati_image_1497.jpg      Vulgar"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_c3.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58d59c-c0f5-4757-9719-4c6d0b2e49ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4463229f-0b88-4541-891c-f607686c8447",
   "metadata": {},
   "source": [
    "# task_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5308a-0ef9-484a-8fee-abbfc4b81cf0",
   "metadata": {},
   "source": [
    "# Handling Pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e88117d-97b5-4904-910e-3f375f71ec2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1618.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકર). ટીચર તમાર તાજમહેલ\\r\\n\\r\\nદેખ/ય છે.\\r\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_31.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરો : ના.\\r\\n છોકરી : કેમ?\\r\\n \\r\\n છોકરી : ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1144.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરીઓ ગમે તેટલી\\r\\n ચાલક હોય,\\r\\n \\r\\n પણ છોક...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1184.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>દોસ્તી કરો,પ્રેમ કરો, વફા કરો...\\r\\n અને બહુ મ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1643.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>છોકરીઓ ગમે તેટલી\\r\\nચાલક હોય,\\r\\n\\r\\nપણ છોકરા ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id     task_1  task_3   task_4  \\\n",
       "0  Gujarati_image_1618.jpg  Sarcastic  Vulgar  Abusive   \n",
       "1    Gujarati_image_31.jpg  Sarcastic  Vulgar  Abusive   \n",
       "2  Gujarati_image_1144.jpg  Sarcastic  Vulgar  Abusive   \n",
       "3  Gujarati_image_1184.jpg  Sarcastic  Vulgar  Abusive   \n",
       "4  Gujarati_image_1643.jpg  Sarcastic  Vulgar  Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0  છોકર). ટીચર તમાર તાજમહેલ\\r\\n\\r\\nદેખ/ય છે.\\r\\n\\...  \n",
       "1  છોકરો : ના.\\r\\n છોકરી : કેમ?\\r\\n \\r\\n છોકરી : ...  \n",
       "2  છોકરીઓ ગમે તેટલી\\r\\n ચાલક હોય,\\r\\n \\r\\n પણ છોક...  \n",
       "3  દોસ્તી કરો,પ્રેમ કરો, વફા કરો...\\r\\n અને બહુ મ...  \n",
       "4  છોકરીઓ ગમે તેટલી\\r\\nચાલક હોય,\\r\\n\\r\\nપણ છોકરા ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../preprocess_data.csv')\n",
    "data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d078098-6c01-4b56-983e-01e422b08fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_3</th>\n",
       "      <th>task_4</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Non Vulgar</td>\n",
       "      <td>Non-abusive</td>\n",
       "      <td>॥વિંદેશીગામડિયો\\r\\n અ |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>ટીચર : સૌથી વધારે દુખાવો ક્યારે\\r\\nથાય?\\r\\nછોક...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>પતિ: તુંમને જરાય પ્રેમ\\r\\nનથી કરતી...\\r\\n\\r\\nપ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>આખા ગોમ ના લોડા\\r\\nભોસ મા ભરી ને બેઠી\\r\\nહોય અ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Non-Sarcastic</td>\n",
       "      <td>Vulgar</td>\n",
       "      <td>Abusive</td>\n",
       "      <td>મિનરલ વોટર સિવાય ક્યારેય\\r\\nબીજું\\r\\nપાણી નો પ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id         task_1      task_3       task_4  \\\n",
       "0  Gujarati_image_1225.jpg  Non-Sarcastic  Non Vulgar  Non-abusive   \n",
       "1  Gujarati_image_1583.jpg      Sarcastic      Vulgar      Abusive   \n",
       "2  Gujarati_image_1502.jpg      Sarcastic      Vulgar      Abusive   \n",
       "3  Gujarati_image_1487.jpg      Sarcastic      Vulgar      Abusive   \n",
       "4  Gujarati_image_1497.jpg  Non-Sarcastic      Vulgar      Abusive   \n",
       "\n",
       "                                          text_clean  \n",
       "0                            ॥વિંદેશીગામડિયો\\r\\n અ |  \n",
       "1  ટીચર : સૌથી વધારે દુખાવો ક્યારે\\r\\nથાય?\\r\\nછોક...  \n",
       "2  પતિ: તુંમને જરાય પ્રેમ\\r\\nનથી કરતી...\\r\\n\\r\\nપ...  \n",
       "3  આખા ગોમ ના લોડા\\r\\nભોસ મા ભરી ને બેઠી\\r\\nહોય અ...  \n",
       "4  મિનરલ વોટર સિવાય ક્યારેય\\r\\nબીજું\\r\\nપાણી નો પ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../preprocess_test_data.csv') \n",
    "test_data.drop(['task_2','text'], axis=1, inplace=True)\n",
    "test_data = test_data.drop(['Unnamed: 0'],axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "547141cb-7934-44ce-8129-6923705cf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data['text_clean'].astype(str)\n",
    "tokenizer = Tokenizer(num_words = 1500,split=' ')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequence = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71148122-b373-4107-81d8-31a9d7fe17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text_clean'].astype(str)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79afc0ac-4007-4918-a471-620671ae9f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words :  7000\n",
      "[[   0    0    0 ...  536  134    1]\n",
      " [   0    0    0 ...  246 1062  318]\n",
      " [   0    0    0 ... 1067 1068  174]\n",
      " ...\n",
      " [   0    0    0 ...    0    0    5]\n",
      " [   0    0    0 ...    0    0    5]\n",
      " [   0    0    0 ...    1   17  433]]\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 2500\n",
    "\n",
    "index_of_words = tokenizer.word_index\n",
    "print(\"No of unique words : \",len(index_of_words))\n",
    "\n",
    "X = pad_sequences(sequence , maxlen = max_seq_len )\n",
    "Y = data['task_4']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee83db9-8b26-488b-90c1-e3f75bf6fded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0 648]\n",
      " [  0   0   0 ... 206 394   1]\n",
      " [  0   0   0 ...   2   1   1]\n",
      " ...\n",
      " [  0   0   0 ...   1   1 324]\n",
      " [  0   0   0 ...  40   2   1]\n",
      " [  0   0   0 ...   0   0   5]]\n"
     ]
    }
   ],
   "source": [
    "test_X = pad_sequences(test_sequence , maxlen = max_seq_len )\n",
    "test_Y = test_data['task_4']\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbdaa2d-a933-4a84-b512-3f3e21230528",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "vocabSize = len(index_of_words)\n",
    "lstm_out = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a59c1c7-81f5-4003-8681-0cb11c980afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, random_state = 0)\n",
    "Y_true = Y_test\n",
    "Y_train = pd.get_dummies(Y_train).values\n",
    "Y_test = pd.get_dummies(Y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b704a4b-3dc4-4373-b1aa-c33c2c3a91ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Y: [[False  True]\n",
      " [ True False]\n",
      " [ True False]\n",
      " ...\n",
      " [False  True]\n",
      " [False  True]\n",
      " [False  True]]\n"
     ]
    }
   ],
   "source": [
    "test_Y_true = test_Y\n",
    "test_Y = pd.get_dummies(test_Y).values\n",
    "print(\"test_Y:\", test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fda6b-baf4-4e18-9aac-4ff2597b2f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e70afe-61cf-4bcb-ad45-892760a4ad6e",
   "metadata": {},
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0695a9f4-3572-472e-8020-d68a81613288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         1792000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1874306 (7.15 MB)\n",
      "Trainable params: 1874306 (7.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim,input_length = 2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24b6c3a8-ffc9-4473-abf9-9c724026b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_a4.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "960541f5-b15c-46c4-a119-a1747b88cc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.7987\n",
      "Epoch 1: val_loss improved from inf to 0.38842, saving model to hasoc_a4.h5\n",
      "24/24 [==============================] - 99s 4s/step - loss: 0.5170 - accuracy: 0.7987 - val_loss: 0.3884 - val_accuracy: 0.8507\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.4228 - accuracy: 0.8066\n",
      "Epoch 2: val_loss improved from 0.38842 to 0.36340, saving model to hasoc_a4.h5\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.4228 - accuracy: 0.8066 - val_loss: 0.3634 - val_accuracy: 0.8507\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.8834\n",
      "Epoch 3: val_loss improved from 0.36340 to 0.33369, saving model to hasoc_a4.h5\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.3419 - accuracy: 0.8834 - val_loss: 0.3337 - val_accuracy: 0.8806\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9311\n",
      "Epoch 4: val_loss did not improve from 0.33369\n",
      "24/24 [==============================] - 98s 4s/step - loss: 0.2415 - accuracy: 0.9311 - val_loss: 0.3399 - val_accuracy: 0.8881\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9550\n",
      "Epoch 5: val_loss did not improve from 0.33369\n",
      "24/24 [==============================] - 96s 4s/step - loss: 0.1628 - accuracy: 0.9550 - val_loss: 0.3967 - val_accuracy: 0.8731\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9682\n",
      "Epoch 6: val_loss did not improve from 0.33369\n",
      "24/24 [==============================] - 98s 4s/step - loss: 0.1214 - accuracy: 0.9682 - val_loss: 0.5032 - val_accuracy: 0.8657\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9642\n",
      "Epoch 7: val_loss did not improve from 0.33369\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.1230 - accuracy: 0.9642 - val_loss: 0.4766 - val_accuracy: 0.8582\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9709\n",
      "Epoch 8: val_loss did not improve from 0.33369\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.1104 - accuracy: 0.9709 - val_loss: 0.5067 - val_accuracy: 0.8657\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9735\n",
      "Epoch 9: val_loss did not improve from 0.33369\n",
      "24/24 [==============================] - 98s 4s/step - loss: 0.0917 - accuracy: 0.9735 - val_loss: 0.5538 - val_accuracy: 0.8507\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9695\n",
      "Epoch 10: val_loss did not improve from 0.33369\n",
      "24/24 [==============================] - 98s 4s/step - loss: 0.1015 - accuracy: 0.9695 - val_loss: 0.4754 - val_accuracy: 0.8582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x224af1d28b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd27b6c-6d1a-4c56-b2ef-7d2df083272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 458ms/step - loss: 0.3337 - accuracy: 0.8806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3336866497993469, 0.8805969953536987]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_a4.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc47188e-1bde-4227-94be-40388fb1dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 10s 512ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9afe3acc-b75f-4b30-8823-935942d1bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Non-abusive':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f33b1b37-22c0-4cac-86e2-e75b16126487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.19      0.31       205\n",
      "           1       0.70      0.98      0.82       399\n",
      "\n",
      "    accuracy                           0.71       604\n",
      "   macro avg       0.76      0.58      0.56       604\n",
      "weighted avg       0.74      0.71      0.64       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26875c0d-3eaa-4a5a-8d93-c006fee19d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred: [[0.00141402 0.99858606]\n",
      " [0.8391441  0.16085593]\n",
      " [0.30731696 0.69268304]\n",
      " ...\n",
      " [0.05582917 0.9441708 ]\n",
      " [0.00347404 0.99652594]\n",
      " [0.00159899 0.998401  ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Y_pred:\", Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b55c7774-8fbe-487e-a3fc-acc0bfb45f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_class: [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"pred_class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "544fbc73-c033-492c-ad76-1ceff2fd969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Non-abusive')\n",
    "    else :\n",
    "        pred_actual.append('Abusive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b77b58c3-fd94-4eb6-b2e6-5bdd1fd93a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id        label\n",
       "0  Gujarati_image_1225.jpg  Non-abusive\n",
       "1  Gujarati_image_1583.jpg      Abusive\n",
       "2  Gujarati_image_1502.jpg  Non-abusive\n",
       "3  Gujarati_image_1487.jpg  Non-abusive\n",
       "4  Gujarati_image_1497.jpg  Non-abusive"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_a4.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7346d58-d7e3-4f99-a9eb-02ba83d58993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5844f175-f3c0-4cab-a482-3bf320151522",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7125888f-1d2d-4bc4-8982-7dcf864ccfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2500, 256)         1792000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2500, 256)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                82176     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1874562 (7.15 MB)\n",
      "Trainable params: 1874434 (7.15 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding层，增加防止过拟合的Dropout\n",
    "model.add(Embedding(input_dim=vocabSize, output_dim=embed_dim, input_length=2500))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM层，增加recurrent_dropout 和 output_dropout\n",
    "model.add(LSTM(units=lstm_out, dropout=0.3, recurrent_dropout=0.3, return_sequences=False))\n",
    "\n",
    "# Batch Normalization增强泛化\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 全连接层，Softmax输出2分类，建议用categorical_crossentropy\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译\n",
    "optimizer = Adam(learning_rate=0.001)  # 学习率也可调整\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17786407-7d45-4f91-b1f6-994d662e023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_b4.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dadb1c5-ded3-40a7-92ba-789bb7f848c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7174 - accuracy: 0.5748\n",
      "Epoch 1: val_loss improved from inf to 0.57835, saving model to hasoc_b4.h5\n",
      "24/24 [==============================] - 99s 4s/step - loss: 0.7174 - accuracy: 0.5748 - val_loss: 0.5783 - val_accuracy: 0.8507\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - ETA: 0s - loss: 0.4703 - accuracy: 0.8106\n",
      "Epoch 2: val_loss improved from 0.57835 to 0.50496, saving model to hasoc_b4.h5\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.4703 - accuracy: 0.8106 - val_loss: 0.5050 - val_accuracy: 0.8507\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.9166\n",
      "Epoch 3: val_loss improved from 0.50496 to 0.44701, saving model to hasoc_b4.h5\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.3282 - accuracy: 0.9166 - val_loss: 0.4470 - val_accuracy: 0.8507\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9404\n",
      "Epoch 4: val_loss improved from 0.44701 to 0.41452, saving model to hasoc_b4.h5\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.2050 - accuracy: 0.9404 - val_loss: 0.4145 - val_accuracy: 0.8507\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9430\n",
      "Epoch 5: val_loss improved from 0.41452 to 0.40030, saving model to hasoc_b4.h5\n",
      "24/24 [==============================] - 98s 4s/step - loss: 0.1786 - accuracy: 0.9430 - val_loss: 0.4003 - val_accuracy: 0.8507\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9642\n",
      "Epoch 6: val_loss improved from 0.40030 to 0.38297, saving model to hasoc_b4.h5\n",
      "24/24 [==============================] - 99s 4s/step - loss: 0.1520 - accuracy: 0.9642 - val_loss: 0.3830 - val_accuracy: 0.8582\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9576\n",
      "Epoch 7: val_loss did not improve from 0.38297\n",
      "24/24 [==============================] - 100s 4s/step - loss: 0.1483 - accuracy: 0.9576 - val_loss: 0.3842 - val_accuracy: 0.8507\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9642\n",
      "Epoch 8: val_loss improved from 0.38297 to 0.37311, saving model to hasoc_b4.h5\n",
      "24/24 [==============================] - 100s 4s/step - loss: 0.1362 - accuracy: 0.9642 - val_loss: 0.3731 - val_accuracy: 0.8582\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9589\n",
      "Epoch 9: val_loss improved from 0.37311 to 0.35972, saving model to hasoc_b4.h5\n",
      "24/24 [==============================] - 100s 4s/step - loss: 0.1260 - accuracy: 0.9589 - val_loss: 0.3597 - val_accuracy: 0.8582\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9695\n",
      "Epoch 10: val_loss did not improve from 0.35972\n",
      "24/24 [==============================] - 102s 4s/step - loss: 0.1295 - accuracy: 0.9695 - val_loss: 0.3641 - val_accuracy: 0.8657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f509e3ff70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 10, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b271750-8de2-43a5-8292-fbdf51eeaeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 424ms/step - loss: 0.3597 - accuracy: 0.8582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35971522331237793, 0.858208954334259]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_b4.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9aeaa1a-9715-412f-a2f1-8edb6211a957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 9s 473ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27fb4e87-a863-47d2-b852-14740c78284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Non-abusive':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff49a634-f583-4d00-8020-35432cace3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.10      0.19       205\n",
      "           1       0.68      1.00      0.81       399\n",
      "\n",
      "    accuracy                           0.69       604\n",
      "   macro avg       0.82      0.55      0.50       604\n",
      "weighted avg       0.78      0.69      0.60       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "538f9ddf-3972-4653-8814-5c40dfd74ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Non-abusive')\n",
    "    else :\n",
    "        pred_actual.append('Abusive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60a722fa-8aa1-4294-9796-6661ffd2c184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id        label\n",
       "0  Gujarati_image_1225.jpg  Non-abusive\n",
       "1  Gujarati_image_1583.jpg      Abusive\n",
       "2  Gujarati_image_1502.jpg  Non-abusive\n",
       "3  Gujarati_image_1487.jpg  Non-abusive\n",
       "4  Gujarati_image_1497.jpg  Non-abusive"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_b4.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43f8a9-da91-4949-a88b-eab4aa96d99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4bb821c-0595-4328-b37d-708915eae85f",
   "metadata": {},
   "source": [
    "# MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cba7bda-f770-4de6-bcf8-63e9e168fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"hasoc_c4.h5\",\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False, \n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f110d0f2-7ea7-478e-bda6-0131f02d79f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.6711 - accuracy: 0.6755 \n",
      "Epoch 1: val_loss improved from inf to 0.59755, saving model to hasoc_c4.h5\n",
      "12/12 [==============================] - 171s 14s/step - loss: 0.6711 - accuracy: 0.6755 - val_loss: 0.5975 - val_accuracy: 0.6269\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\8888\\Anaconda3\\envs\\pythonProject11\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - ETA: 0s - loss: 0.6163 - accuracy: 0.7007 \n",
      "Epoch 2: val_loss did not improve from 0.59755\n",
      "12/12 [==============================] - 163s 13s/step - loss: 0.6163 - accuracy: 0.7007 - val_loss: 0.6313 - val_accuracy: 0.5896\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.5556 - accuracy: 0.7510 \n",
      "Epoch 3: val_loss improved from 0.59755 to 0.53907, saving model to hasoc_c4.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 0.5556 - accuracy: 0.7510 - val_loss: 0.5391 - val_accuracy: 0.7313\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.4280 - accuracy: 0.8464 \n",
      "Epoch 4: val_loss improved from 0.53907 to 0.52551, saving model to hasoc_c4.h5\n",
      "12/12 [==============================] - 164s 14s/step - loss: 0.4280 - accuracy: 0.8464 - val_loss: 0.5255 - val_accuracy: 0.8134\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.9430 \n",
      "Epoch 5: val_loss did not improve from 0.52551\n",
      "12/12 [==============================] - 160s 13s/step - loss: 0.2968 - accuracy: 0.9430 - val_loss: 0.5680 - val_accuracy: 0.8507\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9563 \n",
      "Epoch 6: val_loss improved from 0.52551 to 0.43790, saving model to hasoc_c4.h5\n",
      "12/12 [==============================] - 166s 14s/step - loss: 0.2236 - accuracy: 0.9563 - val_loss: 0.4379 - val_accuracy: 0.8582\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9669 \n",
      "Epoch 7: val_loss did not improve from 0.43790\n",
      "12/12 [==============================] - 163s 14s/step - loss: 0.1726 - accuracy: 0.9669 - val_loss: 0.5259 - val_accuracy: 0.8507\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9669 \n",
      "Epoch 8: val_loss did not improve from 0.43790\n",
      "12/12 [==============================] - 163s 14s/step - loss: 0.1499 - accuracy: 0.9669 - val_loss: 0.5620 - val_accuracy: 0.8582\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.9722 \n",
      "Epoch 9: val_loss did not improve from 0.43790\n",
      "12/12 [==============================] - 166s 14s/step - loss: 0.1416 - accuracy: 0.9722 - val_loss: 0.5877 - val_accuracy: 0.8582\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9656 \n",
      "Epoch 10: val_loss did not improve from 0.43790\n",
      "12/12 [==============================] - 163s 14s/step - loss: 0.1458 - accuracy: 0.9656 - val_loss: 0.5856 - val_accuracy: 0.8507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2466b745e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, embed_dim, input_length=2500))\n",
    "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# 假设Y_train已独热编码\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.argmax(Y_train, axis=1))\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=64, class_weight=class_weight_dict, validation_data=(X_test,Y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "446fc725-847b-4c88-88a6-ea8cc55cb786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 4s 695ms/step - loss: 0.4379 - accuracy: 0.8582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.437895804643631, 0.858208954334259]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('hasoc_c4.h5')\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "580a80e8-da64-4443-bfe0-ede781ce45ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 15s 785ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39815853-ca38-453b-844b-94ba5e65f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in test_Y_true:\n",
    "    if i == 'Non-abusive':\n",
    "        y_actual.append(1)\n",
    "    else :\n",
    "        y_actual.append(0)\n",
    "\n",
    "pred_class = []\n",
    "for i in Y_pred:\n",
    "    pred_class.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb6d9dd4-b677-445f-93c5-ef87c4b75730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.35      0.46       205\n",
      "           1       0.73      0.91      0.81       399\n",
      "\n",
      "    accuracy                           0.72       604\n",
      "   macro avg       0.71      0.63      0.64       604\n",
      "weighted avg       0.71      0.72      0.69       604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_actual , pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36dd3e3c-e77c-4a45-bf34-ab69ba79dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_actual = []\n",
    "for i in pred_class:\n",
    "    if i == 1:\n",
    "        pred_actual.append('Non-abusive')\n",
    "    else :\n",
    "        pred_actual.append('Abusive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8ce1162-c7aa-437b-96f9-55348bd895fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gujarati_image_1225.jpg</td>\n",
       "      <td>Non-abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gujarati_image_1583.jpg</td>\n",
       "      <td>Abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gujarati_image_1502.jpg</td>\n",
       "      <td>Abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gujarati_image_1487.jpg</td>\n",
       "      <td>Abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gujarati_image_1497.jpg</td>\n",
       "      <td>Abusive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id        label\n",
       "0  Gujarati_image_1225.jpg  Non-abusive\n",
       "1  Gujarati_image_1583.jpg      Abusive\n",
       "2  Gujarati_image_1502.jpg      Abusive\n",
       "3  Gujarati_image_1487.jpg      Abusive\n",
       "4  Gujarati_image_1497.jpg      Abusive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[[\"_id\"]]\n",
    "test_data[\"label\"] = pred_actual\n",
    "test_data.to_csv('dl_lstm_c4.csv',index=False)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1fd59-19b7-4e04-930f-a24dd89373f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
